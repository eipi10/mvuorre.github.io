[
  {
    "path": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/",
    "title": "How to calculate contrasts from a fitted brms model",
    "description": "Answer more questions with your estimated parameters, without refitting the model.",
    "author": [
      {
        "name": "Matti Vuorre",
        "url": "https://vuorre.netlify.com"
      }
    ],
    "date": "2020-02-06",
    "categories": [
      "statistics",
      "tutorial",
      "R",
      "brms"
    ],
    "contents": "\n\nContents\nModels and contrasts\nExample data\nModel\nInterpreting the model’s parameters\n\nhypothesis()\nMore contrasts\nDirectional hypotheses and posterior probabilities\nMultiple hypotheses\nHierarchical hypotheses\n\nConclusion\nSupport this work\nSoftware used\n\n\nbrms (Bayesian Regression Models using Stan) is an R package that allows fitting complex (multilevel, multivariate, mixture, …) statistical models with straightforward R modeling syntax, while using Stan for bayesian inference under the hood. You will find many uses of that package on this blog. I am particularly fond of brms’ helper functions for post-processing (visualizing, summarizing, etc) the fitted models. In this post, I will show how to calculate and visualize arbitrary contrasts (aka “(general linear) hypothesis tests”) with brms, with full uncertainty estimates.\nModels and contrasts\nHere, we will discuss linear models, which regress an outcome variable on a weighted combination of predictors, while allowing the weights to vary across individuals (hierarchical linear regression). After fitting the model, you will have estimates of the weights (“beta weights,” or simply regression parameters) that typically consist of an intercept (estimated level of outcome variable when all predictors are zero) and slopes, which indicate how the outcome variable changes as function of one-unit changes of the predictors, when other predictors are at 0.\nHowever, we are often interested in further questions (contrasts, “general linear hypothesis tests”). For example, your model output may report one group’s change over time, and the difference of that slope between groups, but you are particularly interested in the other group’s slope. To find that slope, you’d need to calculate an additional contrast from your model. This is also commonly called “probing interactions” or sometimes “post hoc testing.”\nExample data\nTo make this concrete, let’s consider a hypothetical example data set from Bolger and Laurenceau (2013): Two groups’ (treatment: 0/1) self-reported intimacy was tracked over 16 days (time). The dataset contains data from a total of 50 (simulated) individuals.\n\n\nlibrary(tidyverse)\nlibrary(rio)\ndat <- import(\n  \"http://www.intensivelongitudinal.com/ch4/ch4R.zip\", \n  setclass = \"tibble\", \n  colClasses = c(\"id\" = \"factor\", \"treatment\" = \"factor\")\n)\n\n\n\n\n\n\nModel\nWe might be interested in how the two groups’ feelings of intimacy developed over time, and how their temporal trajectories of intimacy differed. To be more specific, we have three questions:\nQ1: How did intimacy develop over time for group 0? Q2: How did intimacy develop over time for group 1? Q3: How different were these two time-courses?\nTo answer, we model intimacy as a function of time, treatment, and their interactions. The hierarchical model includes varying intercepts and effects of time across participants.\n\n\nlibrary(brms)\nfit <- brm(\n  intimacy ~ time * treatment + (time | id),\n  family = gaussian(),\n  data = dat,\n  file = \"intimacymodel\"\n)\n\n\n\nInterpreting the model’s parameters\nLet’s then answer our questions by looking at the model’s summary, and interpreting the estimated population-level parameters (the posterior means and standard deviations).\n\n\nTable 1: Summary of the Intimacy model’s parameters\n\n\nParameter\n\n\nEstimate\n\n\nEst.Error\n\n\nQ2.5\n\n\nQ97.5\n\n\nb_Intercept\n\n\n2.90\n\n\n0.22\n\n\n2.48\n\n\n3.33\n\n\nb_time\n\n\n0.05\n\n\n0.02\n\n\n0.00\n\n\n0.10\n\n\nb_treatment1\n\n\n-0.05\n\n\n0.31\n\n\n-0.66\n\n\n0.54\n\n\nb_time:treatment1\n\n\n0.06\n\n\n0.03\n\n\n0.00\n\n\n0.13\n\n\nThe first lesson is that most models are simply too complex to interpret by just looking at the numerical parameter estimates. Therefore, we always draw figures to help us interpret what the model thinks is going on. The figure below shows example participants’ data (left) and the model’s estimated effects on the right.\n\n\n\nThen, we can begin interpreting the parameters. First, the intercept indicates estimated intimacy when time and treatment were at their respective baseline levels (0). It is always easiest to interpret the parameters by eyeballing the right panel of the figure above and trying to connect the numbers to the figure. This estimate is the left-most point of the red line.\nThe estimated time parameter describes the slope of the red line (Q1); treatment1 is the difference between the two lines at time zero (Q3). However, we cannot immediately answer Q2 from the parameters, although we can see that the slope of the blue line is about 0.05 + 0.06. To get the answer to Q2, or more generally, any contrast or “general linear hypothesis test” from a brms model, we can use the hypothesis() method.\nhypothesis()\nhypothesis() truly is an underappreciated method of the brms package. It can be very useful in probing complex models. It allows us to calculate, visualize, and summarize, with full uncertainty estimates, any transformation of the model’s parameters. These transformations are often called “contrasts” or “general linear hypothesis tests.” But really, they are just transformations of the joint posterior distribution of the model’s parameters.\nTo answer Q2, then, we encode our question into a combination of the models parameters:\n\n\nq2 <- c(q2 = \"time + time:treatment1 = 0\")\n\n\n\nThe slope of group 1 is calculated from the model’s parameters by adding the slope of group 0 (time) and the interaction term time:treatment1. = 0 indicates that we are interested in contrasting the resulting estimate the zero (“testing against zero” or even “testing the null hypothesis”). Then, we pass this named string to hypothesis(), and observe the results.\n\n\nq2_answer <- hypothesis(fit, q2)\nq2_answer\n\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q2     0.11      0.02     0.06     0.16         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nThe output indicates that the estimated answer to Question 2 is 0.11 with a standard error of 0.02. I will return to Evid.Ratio and Post.Prob shortly.\nThe results can also be visualized.\n\n\nplot(q2_answer)\n\n\n\n\nThat figure shows the (samples from the) posterior distribution of the answer to Question 2.\nMore contrasts\nWith hypothesis() you can answer many additional questions about your model, beyond the parameter estimates. To illustrate, say we are interested in the groups’ difference in intimacy at the end of the study (day 15; Question 4). (The difference at time 0 is reported by the group parameter.)\n\n\nq4 <- c(q4 = \"treatment1 + time:treatment1 * 15 = 0\")\nhypothesis(fit, q4)\n\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q4     0.87      0.41     0.04     1.69         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nDirectional hypotheses and posterior probabilities\nWe can also ask for directional questions. For example, what is the probability that group 0’s slope is greater than 0 (Q5)?\n\n\nq5 <- c(q5 = \"time > 0\")\nq5_answer <- hypothesis(fit, q5)\nq5_answer\n\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q5     0.05      0.02     0.01     0.09      52.33      0.98    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nplot(q5_answer)\n\n\n\n\nWe can now return to Evid.Ratio and Post.Prob: The latter indicates the posterior probability that the parameter of interest is greater than zero (> 0). (More accurately, the proportion of samples from the posterior that are greater than zero.) That should correspond to what you see in the figure above. The former is the ratio of the hypothesis and its complement (the ratio of time > 0 and time < 0). I find posterior probabilities more intuitive than evidence ratios, but they both return essentially the same information. Perhaps of interest, with uniform priors, posterior probabilities will exactly correspond (numerically, not conceptually) to frequentist one-sided p-values (Marsman & Wagenmakers, 2017).\nMultiple hypotheses\nYou can evaluate multiple hypotheses in one function call:\n\n\nhypothesis(fit, c(q2, q4, q5))\n\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1         q2     0.11      0.02     0.06     0.16         NA        NA    *\n2         q4     0.87      0.41     0.04     1.69         NA        NA    *\n3         q5     0.05      0.02     0.01     0.09      52.33      0.98    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nHierarchical hypotheses\nUp to this point, we have “tested” the model’s population level effects. (Parameters for the average person. “Fixed effects.”) Because we fit a hierarchical model with varying intercepts and slopes of time, we can also test the individual specific parameters. For example, we can look at every individual’s estimated intercept (intimacy at time 0):\n\n\nx <- hypothesis(fit, \"Intercept = 0\", group = \"id\", scope = \"coef\")\n\n\n\nIn the above, we asked for the results of the hypothesis test, split by group id (which is the grouping factor in our hierarchical model), and indicated coef as the scope. The latter means that the estimates are the subject-specific deviations with the fixed effect added, as opposed to ranef, which are zero-centered.\nThe results of this question would be a bit too much information to print on screen, so instead we will draw a figure:\n\n\n\nConclusion\nWhen you find that you have a brms model whose parameters don’t quite answer your questions, hypothesis() will probably give you the answer. For more advanced post-processing of your models, I recommend taking a look at the tidybayes package.\n\nSupport this work\n\nSoftware used\nThe following software packages were used in this blog post: R [Version 4.0.3; R Core Team (2020)] and the R-packages brms [Version 2.14.4; Bürkner (2017); Bürkner (2018)], dplyr [Version 1.0.4; Wickham et al. (2021)], forcats [Version 0.5.1; Wickham (2021a)], ggplot2 [Version 3.3.3; Wickham (2016)], kableExtra [Version 1.3.4; Zhu (2021)], knitr [Version 1.31; Xie (2015)], patchwork [Version 1.1.1; Pedersen (2020)], purrr [Version 0.3.4; Henry and Wickham (2020)], Rcpp [Version 1.0.6; Eddelbuettel and François (2011); Eddelbuettel and Balamuta (2018)], readr [Version 1.4.0; Wickham and Hester (2020)], rio [Version 0.5.26; Chan et al. (2021)], scales [Version 1.1.1; Wickham and Seidel (2020)], stringr [Version 1.4.0; Wickham (2019)], tibble [Version 3.1.0; Müller and Wickham (2021)], tidyr [Version 1.1.3; Wickham (2021b)], and tidyverse [Version 1.3.0; Wickham et al. (2019)].\n\n\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\n———. 2018. “Advanced Bayesian Multilevel Modeling with the R Package brms.” The R Journal 10 (1): 395–411. https://doi.org/10.32614/RJ-2018-017.\n\n\nChan, Chung-hong, Geoffrey CH Chan, Thomas J. Leeper, and Jason Becker. 2021. Rio: A Swiss-Army Knife for Data File i/o.\n\n\nEddelbuettel, Dirk, and James Joseph Balamuta. 2018. “Extending extitR with extitC++: A Brief Introduction to extitRcpp.” The American Statistician 72 (1): 28–36. https://doi.org/10.1080/00031305.2017.1375990.\n\n\nEddelbuettel, Dirk, and Romain François. 2011. “Rcpp: Seamless R and C++ Integration.” Journal of Statistical Software 40 (8): 1–18. https://doi.org/10.18637/jss.v040.i08.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nMüller, Kirill, and Hadley Wickham. 2021. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nPedersen, Thomas Lin. 2020. Patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2021a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2021b. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, and Dana Seidel. 2020. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nXie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\nZhu, Hao. 2021. kableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra.\n\n\n\n\n",
    "preview": "posts/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model/2020-02-06-how-to-calculate-contrasts-from-a-fitted-brms-model_files/figure-html5/figure-1.png",
    "last_modified": "2021-03-09T20:51:50+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/",
    "title": "How to analyze visual analog (slider) scale data?",
    "description": "A reasonable choice might be the zero-one-inflated beta model",
    "author": [
      {
        "name": "Matti Vuorre",
        "url": "https://vuorre.netlify.com"
      }
    ],
    "date": "2019-02-18",
    "categories": [
      "psychology",
      "statistics",
      "tutorial",
      "R",
      "brms"
    ],
    "contents": "\n\nContents\nIntroduction\nNormal model of slider ratings\nToward a better model\n\nThe zero-one-inflated beta model\nThe beta distribution\nZero-one inflation\n\nZOIB regression\nExample data\nThe model\n\nSimulation: Compare ZOIB and t-test performances\nDiscussion\nLimitations\nFurther reading\nSupport this work\nSoftware used\n\n\n\nIntroduction\nIn psychological experiments, subjective responses are often collected using two types of response scales: ordinal and visual analog scales. These scales are unlikely to provide normally distributed data. However, researchers often analyze responses from these scales with models that assume normality of the data.1\nOrdinal scales, of which binary ratings are a special case, provide ordinal data and are thus better analyzed using ordinal models (Bürkner and Vuorre 2019; Liddell and Kruschke 2018).\nAnalog scales, also known as slider scales, are also unlikely to provide normally distributed responses because the scale is bounded at the low and high ends. These responses also tend to be skewed. It is common for slider responses to bunch at either end of the slider scale, potentially making the deviation from normality more severe.\nFor example, Figure 1 shows a slider scale in action. (I found this random example with a simple internet search at https://blog.surveyhero.com/2018/09/03/new-question-type-slider/). In experiments using slider scales, subjects are typically instructed to use their mouse to drag a response indicator along a horizontal line, and/or click with a mouse on a point of the scale that matches their subjective impression. Sometimes these responses are provided on paper, where subjects are asked to bisect a line at a point that matches their subjective feeling (e.g. halfway between “Leisure” and “Money” if they are subjectively equally important.)\n\n\n\nFigure 1: Example slider scale from https://blog.surveyhero.com/2018/09/03/new-question-type-slider/\n\n\n\nThese analog ratings are sometimes thought to be ‘better’ than discrete ordinal ratings (Likert item responses) because of the greater resolution of the slider scale. The scale’s resolution is limited only by the resolution of the monitor: For example, if the rating scale is 100 pixels wide, there are 100 possible values for the ratings. It is not unthinkable that such ratings can be considered continuous between the low and high endpoints. However, they are often not well described by the normal distribution.\nNormal model of slider ratings\nConsider Figure 2. This figure shows 200 simulated ratings on a [0, 1] slider scale (meaning that any value between 0 and 1, inclusive of the endpoints, is possible). I have also superimposed a blue curve of the best-fitting normal density on the histogram. The two most notable non-normal features of these data are that they are bounded at 0 and 1 where the data appears to “bunch,” and (possibly) skewed. Of course, these data were simulated; experience with slider scales tells me, however, that this histogram is not unrepresentative of such ratings.\n\n\n\nFigure 2: Histogram of 200 simulated slider scale ratings, with a superimposed best-fitting density curve from a normal distribution.\n\n\n\nWhile the height of the blue curve is not comparable to the heights of the bars (one represents a density, the other counts of observations in rating bins), it should be apparent that features of the rating scale data make the blue normal curve a poor representation of the data.\nFirst, the skew apparent in the data is not captured by the normal density curve. Second, and perhaps more important, the blue curve does not respect the 0 and 1 boundaries of the slider scale data.\nFocus on this latter point: We can see that the blue curve assigns density to areas outside the possible values: The model predicts impossible values with alarming frequency. Second, the boundary values 0.0 and 1.0 do not receive any special treatment under the normal model, but we can see that the data are bunched at the boundaries. The great frequency of responses at 0.0 and 1.0 leads to large prediction errors from the normal model of these data.\nIn other words, (simulated) subjects tend to give many extreme ratings. This is especially apparent in the low end of the rating scale, where the continuous spread of scores tapers off, but then there is a large spike of ratings at zero. The normal model misses these features of the data, and may therefore lead to unrepresentative estimates of the data generating process, and even erroneous conclusions.\nToward a better model\nMore generally, if your goal is to predict cognition and behavior (Yarkoni and Westfall 2017), a model that is obviously a poor representation of your data—in terms of having such a poor predictive utility—should not be your first choice for data analysis.\nAdmittedly, the data in Figure 2 were simulated, and it remains an empirical question as to how common these features are in real data, and how severe these issues are to normal models (t-test, ANOVA, correlation, etc.).\nNevertheless, it would be desirable to have an accessible data-analytic model for slider scale data, whose assumption better match observed features of the data. Here, I introduce one such model—the zero-one-inflated beta (ZOIB) model—and show how it can be applied to real data using the R package brms (Bürkner 2017b). I also compare this model to standard analyses of slider scale data and conclude that the ZOIB can provide more detailed and accurate inferences from data than its conventional counterparts.\n\n\n\nFigure 3: Dr. John A. Zoidberg thinks you should try a ZOIB model on your slider scale data.\n\n\n\nThe zero-one-inflated beta model\nAbove, we established—rather informally—that normal models may be less than optimal for slider scale data. Of course, no model is the correct model of such data, but it would be desirable to use a model that best represents the data under study.\nThe model for analysis of slider scale data discussed here has been called the “zero-one-inflated beta” model, or ZOIB (Liu and Kong 2015). It is a model of data in the closed [0, 1] interval, and has two components: A beta distribution for responses in the closed (0, 1) interval, and a bernoulli distribution for the binary {0, 1} responses. Under this model, predictors can affect either or both the continuous and binary responses, the proportion of binary responses, or the spread of the continuous ratings.\nTo understand ZOIB, let’s start with a closer look at the theoretical beta density.\nThe beta distribution\nThe beta distribution used in beta regression (Ferrari and Cribari-Neto 2004) is a model of data in the open (0, 1) interval. (i.e. all values from 0 to 1, but not 0 and 1 themselves, are permitted.)\nThe beta distribution typically has two parameters, which in R are called shape1 and shape2. Together, they determine the location, spread, and skew of the distribution. Four example beta densities are shown in Figure 4. Using R’s dbeta(), I drew four curves corresponding to beta densities with different shape1 and 2 parameters.\n\n\n\nFigure 4: Four examples of the beta density, corresponding to different shape parameters.\n\n\n\nThis default parameterization is useful, for example, as a prior distribution for proportions: The shape1 and shape2 parameters can define the prior number of zeros and ones, respectively. For example, in the above figure, dbeta(x, shape1 = 1, shape2 = 1) results in a uniform prior over proportions, because the prior zeros and ones are 1 each.\nHowever, for our purposes, it is more useful to parameterize the beta distribution with a mean and a precision. To convert the former parameterization to mean (which we’ll call \\(\\mu\\) (mu)) and precision (\\(\\phi\\) (phi)), the following formulas can be used\n\\[\n\\mbox{shape1} = \\mu \\phi \\\\ \\mbox{shape2} = (1 - \\mu)\\phi\n\\]\n(This parameterization is provided in R in the PropBeta functions from the extraDist package, which calls the precision parameter, or \\(\\phi\\), size.) Redrawing the figure from above with this parameterization using the dprop() function, we get Figure 5.\n\n\n\nFigure 5: Four examples of the reparameterized beta density (dprop()).\n\n\n\nShown above are four density functions of the beta family, whose precision and mean are varied. The first (red line) is a beta distribution with precision = 1, and mean = 0.5. It results in a uniform distribution. If a subject gave random slider scale responses, they might look much like this distribution (any rating is equally probably as any other rating).\nThe second beta distribution (green line) has precision 10, and mean 0.2. It is heavily skewed to the right. The third distribution (teal line) has precision 10, and a mean of 0.9. The fourth one, most similar to a normal distribution, has precision 70 and mean 0.50 (purple line).\nIn beta regression, this family of distributions is used to model observations, and covariates can have effects on both the mean and precision parameters.\nHowever, beta regression only allows outcomes in the open (0, 1) interval. We know that slider scales often result in a bunching of values at the boundaries, and these boundary values might be informative of the participants’ cognition and behavior. To handle these extreme values, we can add a zero-one inflation process to the beta distribution.\nZero-one inflation\nThe zero-one-inflated beta (ZOIB) adds a separate discrete process for the {0, 1} values, using two additional parameters. Following convention, we shall call them \\(\\alpha\\) (alpha) and \\(\\gamma\\) (gamma). These parameters describe the probability of an observation being a 0 or 1 (\\(\\alpha\\)), and conditional on that, whether the observation was 1 (\\(\\gamma\\)).\nIn other words, the model of outcomes under ZOIB is described by four parameters. The first is \\(\\alpha\\), the probability that an observation is either 0 or 1. (Thus, \\(1-\\alpha\\) is the probability of a non-boundary observation.) If an observation is not 0 or 1, the datum is described by the beta distribution with some mean \\(\\mu\\) and precision \\(\\phi\\). If an observation is 0 or 1, the probability of it being 1 is given by \\(\\gamma\\) (just like your usual model of binary outcomes, e.g. logistic regression). So you can think of the model as a kind of mixture of beta and logistic regressions, where the \\(\\alpha\\) parameter describes the mixing proportions. The mathematical representation of this model is given in this vignette (Bürkner 2017b).\nTo illustrate, I wrote a little function rzoib() that takes these parameters as arguments, and generates n random draws. Here is a histogram of 1k samples from four ZOIB distributions with various combinations of the parameters:\n\n\n\nFigure 6: Four different ZOIB distributions resulting from various combinations of the parameters. (Parameter names are abbreviated; a = alpha, g = gamma, etc.)\n\n\n\nTake the first (red) one. \\(\\alpha\\) was set to zero, and therefore there are no observations exactly at zero or 1. Because \\(\\alpha = 0\\), it doesn’t matter that \\(\\gamma\\) was set to 0.5. \\(\\gamma\\) is the conditional one probability, given that the observation was 0 or 1. Therefore, the first histogram only contains draws from a beta distribution with mean = 0.2, and precision = 6.\nNext, take a look at the second (green) histogram. Here, \\(\\alpha = 0.1\\), so 10% of the observations will be either 0 or 1. Of these 10%, 30% are ones (\\(\\gamma = 0.3\\)). The bulk of the distribution, 90%, are draws from a beta distribution with a mean = 0.5, and precision = 3.\nThe bottom two histograms are two more combinations of the four parameters. Try to understand how their shapes are explained by the specific parameter combinations.\nIn summary, ZOIB is a reasonable model of slider scale data that can capture their major features, has support for the entire [0, 1] range of data, and does not assign density to impossible values (unlike the normal model). It also has an intuitive way of dealing with the boundary values as a separate process, thus providing more nuanced information about the outcome variable under study.\nNext, we discuss a regression model with ZOIB as the data model: We are most interested in how other variables affect or relate to the outcome variables under study (slider scale ratings). By modeling the four parameters of the ZOIB model on predictors, ZOIB regression allows us to do just that.\nZOIB regression\nIn this example, we examine the ZOIB model in the context of one binary predictor variable (Group A vs B, a “between subjects” manipulation).\nExample data\nTo illustrate the ZOIB model in action, I simulated a data set of 100 ratings from two groups, A and B. These data are shown in Figure 7.\n\n\n\nFigure 7: Simulated data set of two group’s slider scale ratings, with means and bootstrapped 95% CIs in blue. The ratings are jittered horizontally to reveal overlapping data points.\n\n\n\n\n\n\n\n\n\nWe are interested in the extent to which Group A’s ratings differ from Group B’s ratings. It is common practice to address this question with a t-test, treating the ratings as normally distributed within each group. I compared the two groups’ means with a t-test: The difference was not statistically significant (B - A = 0.06, 95%CI = [-0.07, 0.2], p=0.340). I’ve also heard that you can do something called a Mann-Whitney U test, or a Kruskal-Wallis test when you have a categorical predictor and don’t want to assume a parametric form for your outcomes. I tried those as well. Neither of these nonparametric tests were significant (p=0.226; p=0.225). I therefore concluded that I was unable to reject the null hypothesis that Group A and Group B’s population means are not different.\nBut as can be seen from Figure 2, the normal model makes unreasonable assumptions about these ratings. We see in Figure 7 that there are many non-normal features in this example data set; e.g. many values are bunched at 0.0 and 1.0. Let’s fit the ZOIB model on these data, and see if our conclusions differ. Spoiler alert: they do.\nThe model\nWe will model the data as ZOIB, and use group as a predictor of the mean and precision of the beta distribution, the zero-one inflation probability \\(\\alpha\\), and the conditional one-inflation probability \\(\\gamma\\). In other words, in this model group may affect the mean and/or precision of the assumed beta distribution of the continuous ratings (0, 1), and/or the probability with which a binary rating is given, and/or the probability that a binary rating is 1. How do we estimate this model?\nIt might not come as a surprise that we estimate the model with bayesian methods, using the R package brms (Bürkner 2017b). Previously, I have discussed how to estimate signal detection theoretic models, “robust models,” and other multilevel models using this package. I’m a big fan of brms because of its modeling flexibility and post-processing functions: With concise syntax, you can fit a wide variety of possibly nonlinear, multivariate, and multilevel models, and analyze and visualize the models’ results.\nLet’s load the package, and start building our model.\n\n\nlibrary(brms)\n\n\n\nThe R formula syntax allows a concise representation of regression models in the form of response ~ predictors. For a simple normal (i.e. gaussian) model of the mean of Ratings as a function of group, you could write Ratings ~ group, family = gaussian. However, we want to predict the four parameters of the ZOIB model, and so will need to expand this notation.\nThe brms package allows modeling more than one parameter of an outcome distribution. Specifically, we want to predict so-called “distributional parameters,” and bf() allows predicting them in their own formulas. Implicitly, Ratings ~ group means that you want to model the mean of Ratings on group. Therefore, to model \\(\\phi\\), \\(\\alpha\\), and \\(\\gamma\\), we will give them their own regression formulas within a call to bf():\n\n\nzoib_model <- bf(\n  Rating ~ group,\n  phi ~ group,\n  zoi ~ group,\n  coi ~ group, \n  family = zero_one_inflated_beta()\n)\n\n\n\nThe four sub-models of our model are, in order of appearance: 1. the model of the beta distribution’s mean (read, “predict Rating’s mean from group”). Then, 2. the model of phi; the beta distribution’s precision. 3. zoi is the zero-one inflation (\\(\\alpha\\)); that is, we model the probability of a binary rating as a function of group. 4. coi is the conditional one-inflation: Given that a response was {0, 1}, the probability of it being 1 is modelled on group.\nAs is usual in R’s formula syntax, the intercepts of each of these formulas are implicitly included. (To make intercepts explicit, use e.g. Rating ~ 1 + group.) Therefore, this model will have 8 parameters; the intercepts are Group A’s mean, phi, zoi, and coi. Then, there will be a Group B parameter for each of them, indicating the extent to which the parameters differ for Group B versus Group A.\nIf group has a positive effect on (the mean of) Rating, we may conclude that the continuous rating’s mean differs as function of Group. On the other hand, if coi is affected by group, Group has an effect on the binary {0, 1} ratings. If group has no effects on any of the parameters, we throw up our hands and design a new study.\nFinally, we specified family = zero_one_inflated_beta(). Just like logistic regression, ZOIB regression is a type of generalized linear model. Therefore, each distributional parameter is modeled through a link function. The mean, zoi, and coi parameters are modeled through a logit link function. Phi is modeled through a log link function. These link functions can be changed by giving named arguments to zero_one_inflated_beta(). It is important to keep in mind the specific link functions, we will need them when interpreting the model’s parameters.\nTo estimate this model, we pass the resulting zoib_model to brm(), with a data frame from the current R environment, 4 CPU cores for speed, and a file argument to save the resulting model to disk. The last two arguments are optional.\n\n\nfit <- brm(\n  formula = zoib_model,\n  data = dat,\n  cores = 4,\n  file = \"brm-zoib\"\n)\n\n\n\nbrms estimates the regression model using bayesian methods: It will return random draws from the parameters’ posterior distribution. It takes less than a minute to draw samples from this model. Let’s then interpret the estimated parameters (i.e. the numerical summaries of the posterior distribution):\n\n\nsummary(fit)\n\n\n Family: zero_one_inflated_beta \n  Links: mu = logit; phi = log; zoi = logit; coi = logit \nFormula: Rating ~ group \n         phi ~ group\n         zoi ~ group\n         coi ~ group\n   Data: dat (Number of observations: 100) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         0.33      0.16     0.03     0.64 1.00     5984     2997\nphi_Intercept     1.49      0.24     1.00     1.92 1.00     5941     3217\nzoi_Intercept    -0.81      0.32    -1.45    -0.20 1.00     6959     3025\ncoi_Intercept     0.61      0.56    -0.44     1.76 1.00     5929     2712\ngroupB            0.91      0.21     0.50     1.30 1.00     6038     2969\nphi_groupB        0.49      0.33    -0.14     1.12 1.00     5415     3211\nzoi_groupB        0.09      0.42    -0.74     0.88 1.00     6376     2899\ncoi_groupB       -0.85      0.75    -2.29     0.58 1.00     6368     2970\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nFirst, the summary of this model prints a paragraph of information about the model, such as the outcome family (ZOIB), link functions, etc. The regression coefficients are found under the “Population-Level Effects:” header. The columns of this section are “Estimate,” the posterior mean or point estimate of the parameter. “Est.Error,” the posterior standard deviation, or so called standard error of the parameter. Then, the lower and upper limit of the 95% Credible Interval. The two last columns are diagnostics of the model fitting procedure.\nThe first four rows of this describe the parameters for the baseline group (Group A). Intercept is the logit-transformed mean of the beta distribution for Group A’s ratings (the subset of ratings that were (0, 1)). Next, phi_Intercept describes the precision of the beta distribution fitted to Group A’s slider responses, on the scale of the (log) link function. zoi_Intercept is the zero or one inflation of Group A’s data, on the logit scale. coi_Intercept is the conditional one inflation; out of the 0 or 1 ratings in Group A’s data, describing the proportion of ones (out of the 0/1 responses)?\nThese parameters are described on the link scale, so for each of them, we can use the inverse link function to transform them to the response scale. Precision (phi_Intercept) was modeled on the log scale. Therefore, we can convert it back to the original scale by exponentiating. For the other parameters, which were modeled on the logit scale, we can use the inverse, which is plogis().\nHowever, before converting the parameters, it is important to note that the estimates displayed above are summaries (means, quantiles) of the posterior draws of the parameters on the link function scale. Therefore, we cannot simply convert the summaries. Instead, we must transform each of the posterior samples, and then re-calculate the summaries. The following code accomplishes this “transform-then-summarize” procedure for each of the four parameters:\n\n\nposterior_samples(fit, pars = \"b_\")[,1:4] %>% \n  mutate_at(c(\"b_phi_Intercept\"), exp) %>% \n  mutate_at(vars(-\"b_phi_Intercept\"), plogis) %>% \n  posterior_summary() %>% \n  as.data.frame() %>% \n  rownames_to_column(\"Parameter\") %>% \n  kable(digits = 2) \n\n\nParameter\nEstimate\nEst.Error\nQ2.5\nQ97.5\nb_Intercept\n0.58\n0.04\n0.51\n0.66\nb_phi_Intercept\n4.55\n1.07\n2.72\n6.84\nb_zoi_Intercept\n0.31\n0.07\n0.19\n0.45\nb_coi_Intercept\n0.64\n0.12\n0.39\n0.85\n\nWe can then interpret these summaries, beginning with b_Intercept. This is the estimated mean of the beta distribution fitted to Group A’s (0, 1) rating scale responses (with its standard error, lower- and upper limits of the 95% CI). Then, b_Phi_Intercept is the precision of the beta distribution. zoi is the zero-one inflation, and coi the conditional one inflation.\nTo make b_zoi_Intercept concrete, we should be able to compare its posterior mean to the observed proportion of 0/1 values in the data:\n\n\nmean(dat$Rating[dat$group==\"A\"] %in% 0:1) %>% round(3)\n\n\n[1] 0.311\n\nAbove we calculated the proportion of zeros and ones in the data set, and found that it matches the estimated value. Similarly, for coi, we can find the corresponding value from the data:\n\n\nmean(dat$Rating[dat$group==\"A\" & dat$Rating %in% 0:1] == 1) %>% \n  round(3)\n\n\n[1] 0.643\n\nLet’s get back to the model summary output. The following four parameters are the effects of being in group B on these parameters. Most importantly, groupB is the effect of group B (versus group A) on the mean of the ratings’ assumed beta distribution, in the logit scale. Immediately, we can see that the parameter’s 95% Credible Interval does not include zero. Traditionally, this parameter would be called “significant”; group B’s (0, 1) ratings are on average greater than group A’s.\nTo transform this effect back to the data scale, we can again use plogis(). However, it is important to keep in mind that the effect’s size on the original scale depends on the intercept, getting smaller as the intercept increases (just like in any other generalized linear model.) The following bit of code transforms this effect and its uncertainty back to the original scale.\n\n\nh <- c(\"B - A\" = \"plogis(Intercept + groupB) = plogis(Intercept)\")\nhypothesis(fit, h)\n\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1      B - A     0.19      0.04     0.11     0.28         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nThe data were simulated with the rzoib() function, and I set \\(\\alpha = 0.25, \\gamma = 0.5, \\mu = 0.6 + 0.15\\mbox{groupB}, \\phi = 5\\). Therefore, the results of the t-tests and nonparametric tests were misses; a true effect was missed. On the other hand, the ZOIB regression model detected the true effect of group on the beta distribution’s mean.\nFinally, let’s visualize this key finding using the conditional_effects() function from brms.\n\n\nplot(\n  conditional_effects(fit, dpar = \"mu\"), \n  points = TRUE, \n  point_args = list(width = .05, shape = 1)\n)\n\n\n\n\nFigure 8: Estimated mu parameters from the example ZOIB fit, as filled points and error bars (95% CIs), with the original data (empty circles).\n\n\n\nComparing Figure 8 to Figure 7 reveals the fundamental difference of the normal t-test model, and the ZOIB model: The ZOIB regression (8) has found a large difference between the continuous part of the slider ratings’ means because it has treated the data with an appropriate model. By conflating the continuous and binary data, the t-test did not detect this difference.\nIn conclusion, this example showed that ZOIB results in more informative, and potentially more accurate, inferences from analog scale (“slider”) data. Of course, in this simulation we had the benefit of knowing the true state of matters: The data were simulated from a ZOIB model. Nevertheless, we have reasoned that by respecting the major features of slider scale data, the ZOIB is a more accurate representation of it, and was therefore able to detect a difference where the t-test did not. Next, I put this conjecture to a test by conducting a small simulation study.\nSimulation: Compare ZOIB and t-test performances\nTo compare the performance of the t-test and ZOIB in a little bit more detail, I conducted a small simulation study. I simulated 100 data sets of 200 ratings from two independent groups, from the ZOIB model (100 ratings per group). I set \\(\\alpha = 0.2, \\gamma = 0.7, \\mu = 0.6 + 0.1\\mbox{groupB}, \\phi = 5\\); that is, there was a small effect of group on the mean of the beta distribution, and all other parameters were constant across groups. A sample of the resulting data sets is shown in Figure 9\n\n\n\n\n\n\nFigure 9: Six randomly selected simulated data sets. Points are individual ratings (jittered to show overlapping points), while blue symbols indicate the means and bootstrapped 95% CIs.\n\n\n\n\n\n\nI first conducted an independent samples, unequal variances t-test on each of the 100 simulated data sets, comparing the two groups’ mean ratings. 35% of these t-tests were significantly positive at the .05 level. That is, the power of the t-test in this simulation was about 35%. (Uncertainty in this value is moderate, because I only did 100 simulation runs.)\n\n\n\n\n\n\nI then estimated the ZOIB model for each of the 100 simulated data sets. Statistical significance does not play a role in Bayesian statistics, but to most easily compare the results of these two models, I calculated the proportion of simulations for which the estimated Group on \\(\\mu\\) effect’s 95% Credible Interval was entirely above zero. If a 95% CI does not include zero, disrespecting the philosophical differences of bayesian and frequentist statistics, I may say that the estimate is “significant.”\nThis parameter was significantly greater than zero in 65% of the ZOIB models estimated on the same 100 simulated data sets. That is, the power of this model to detect an effect was much greater than the power of the t-test. These results are illustrated in Figure 10.\n\n\n\nFigure 10: Results of the simulation study. Top: the estimated mean difference and 95% CI of the two groups’ ratings, as estimated by a t-test. Red = not statistically significant; blue = statistically significant. The data sets are ordered on the x axis on the estimated mean difference. Bottom: simulation results of the ZOIB model. Same as the top panel, but the estimated parameter is the difference between the two group’s mu parameters of the beta distribution. (I back-transformed the mu parameter from the logit scale to the data scale to make the results numerically more comparable across the t-test and ZOIB models.) In both panels, the horizontal green line indicates the true effect used in the simulations.\n\n\n\nAs can be seen in this figure, in this particular setup, the t-tests severely underperformed in detecting a true effect when compared to the ZOIB model. Of course, this is to be expected, because the data were generated from the ZOIB model.\nOut there in the wild, which of these models is closer to the true data generating process for slider scale ratings? Normal models, or ZOIB? (Or, most likely, some other class of models?) As we have seen, normal models may be poor representations of bounded and skewed slider scale data. It is therefore possible that the routine use of normal models in analyzing slider scale data can result in missing true effects at a rate higher than indicated by conventional power analyses.\nDiscussion\nI have not extensively reviewed the performance of the ZOIB model in this blog post. Neither did I analyze real slider scale data. Therefore, I can not and would not recommend exclusively favoring the ZOIB model over normal models for the analysis of slider scale data. However, I can recommend at least trying ZOIB for your own slider scale data, and thinking about what models might best fit your data if they appear non normal.\nLimitations\nThere are many limitations to the current discussion, and the simulation studies should be considerably expanded to more realistic and variable situations.\nOne limitation of the ZOIB model might be what I here discussed as its main benefit. ZOIB separates the binary and continuous processes, such that a predictor’s effect on one or both of them are independent in the model. However, it is likely that these two processes are somehow correlated. Thus, ZOIB does not give only one “effect” of a predictor on the ratings, but two, one for the continuous part, and one for the binary. By not getting a single effect, if nothing else, the model is more complex and probably more difficult to analyze and/or explain.\nFurther reading\nThe beta regression model has previously been discussed as a reasonable model of data in the open (0, 1) interval (Ferrari and Cribari-Neto 2004). It’s application in psychological studies has also been discussed by (Smithson and Verkuilen 2006; see also Verkuilen and Smithson 2012). These earlier papers recommended that values at the 0 and 1 boundaries be somehow transformed to make the data suitable for the model, but transforming the data such that a model can be fitted seems like a bad idea.\nMixtures of beta and discrete models were discussed by Ospina and Ferrari (2008), and an R package for estimation of the ZOIB model was introduced by Liu and Kong (2015). Liu and Eugenio (2018) found that ZOIB models are better estimated with Bayesian methods than with maximum likelihood methods.\nMore information about the brms package can be found in Bürkner (2017b), and in the excellent vignettes at https://cran.rstudio.com/web/packages/brms/.\n\nSupport this work\n\nSoftware used\nThe following software packages were used: R [Version 4.0.3; R Core Team (2020)] and the R-packages brms [Version 2.14.4; Bürkner (2017a); Bürkner (2018)], broom [Version 0.7.5.9000; Robinson, Hayes, and Couch (2021)], dplyr [Version 1.0.4; Wickham et al. (2021)], extraDistr [Version 1.9.1; Wolodzko (2020)], forcats [Version 0.5.1; Wickham (2021a)], ggbeeswarm [Version 0.6.0; Clarke and Sherrill-Mix (2017)], ggplot2 [Version 3.3.3; Wickham (2016)], glue [Version 1.4.2; Hester (2020)], knitr [Version 1.31; Xie (2015)], patchwork [Version 1.1.1; Pedersen (2020)], purrr [Version 0.3.4; Henry and Wickham (2020)], Rcpp [Version 1.0.6; Eddelbuettel and François (2011); Eddelbuettel and Balamuta (2018)], readr [Version 1.4.0; Wickham and Hester (2020)], scales [Version 1.1.1; Wickham and Seidel (2020)], stringr [Version 1.4.0; Wickham (2019)], tibble [Version 3.1.0; Müller and Wickham (2021)], tidyr [Version 1.1.3; Wickham (2021b)], and tidyverse [Version 1.3.0; Wickham et al. (2019)].\n\n\n\n\nBürkner, Paul-Christian. 2017a. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\n———. 2017b. “Brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\n———. 2018. “Advanced Bayesian Multilevel Modeling with the R Package brms.” The R Journal 10 (1): 395–411. https://doi.org/10.32614/RJ-2018-017.\n\n\nBürkner, Paul-Christian, and Matti Vuorre. 2019. “Ordinal Regression Models in Psychology: A Tutorial.” Advances in Methods and Practices in Psychological Science 2 (1): 77–101. https://doi.org/10.1177/2515245918823199.\n\n\nClarke, Erik, and Scott Sherrill-Mix. 2017. Ggbeeswarm: Categorical Scatter (Violin Point) Plots. https://CRAN.R-project.org/package=ggbeeswarm.\n\n\nEddelbuettel, Dirk, and James Joseph Balamuta. 2018. “Extending extitR with extitC++: A Brief Introduction to extitRcpp.” The American Statistician 72 (1): 28–36. https://doi.org/10.1080/00031305.2017.1375990.\n\n\nEddelbuettel, Dirk, and Romain François. 2011. “Rcpp: Seamless R and C++ Integration.” Journal of Statistical Software 40 (8): 1–18. https://doi.org/10.18637/jss.v040.i08.\n\n\nFerrari, Silvia, and Francisco Cribari-Neto. 2004. “Beta Regression for Modelling Rates and Proportions.” Journal of Applied Statistics 31 (7): 799–815. https://doi.org/10.1080/0266476042000214501.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nHester, Jim. 2020. Glue: Interpreted String Literals. https://CRAN.R-project.org/package=glue.\n\n\nLiddell, Torrin M., and John K. Kruschke. 2018. “Analyzing Ordinal Data with Metric Models: What Could Possibly Go Wrong?” Journal of Experimental Social Psychology 79 (November): 328–48. https://doi.org/10.1016/j.jesp.2018.08.009.\n\n\nLiu, Fang, and Evercita C Eugenio. 2018. “A Review and Comparison of Bayesian and Likelihood-Based Inferences in Beta Regression and Zero-or-One-Inflated Beta Regression.” Statistical Methods in Medical Research 27 (4): 1024–44. https://doi.org/10.1177/0962280216650699.\n\n\nLiu, Fang, and Yunchuan Kong. 2015. “Zoib: An R Package for Bayesian Inference for Beta Regression and Zero/One Inflated Beta Regression.” The R Journal 7 (2): 34–51. https://journal.r-project.org/archive/2015/RJ-2015-019/index.html.\n\n\nMüller, Kirill, and Hadley Wickham. 2021. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nOspina, Raydonal, and Silvia L. P. Ferrari. 2008. “Inflated Beta Distributions.” Statistical Papers 51 (1): 111. https://doi.org/10.1007/s00362-008-0125-4.\n\n\nPedersen, Thomas Lin. 2020. Patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2021. Broom: Convert Statistical Objects into Tidy Tibbles.\n\n\nSmithson, Michael, and Jay Verkuilen. 2006. “A Better Lemon Squeezer? Maximum-Likelihood Regression with Beta-Distributed Dependent Variables.” Psychological Methods 11 (1): 54–71. https://doi.org/10.1037/1082-989X.11.1.54.\n\n\nVerkuilen, Jay, and Michael Smithson. 2012. “Mixed and Mixture Regression Models for Continuous Bounded Responses Using the Beta Distribution.” Journal of Educational and Behavioral Statistics 37 (1): 82–113. https://doi.org/10.3102/1076998610396895.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2021a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2021b. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, and Dana Seidel. 2020. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nWolodzko, Tymoteusz. 2020. extraDistr: Additional Univariate and Multivariate Distributions. https://CRAN.R-project.org/package=extraDistr.\n\n\nXie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\nYarkoni, Tal, and Jacob Westfall. 2017. “Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning.” Perspectives on Psychological Science 12 (6): 1100–1122. https://doi.org/10.1177/1745691617693393.\n\n\nTechnically, normal models assume that the residuals are normally distributed. I will keep referring to data being normally distributed or not, for clarity.↩︎\n",
    "preview": "posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models_files/figure-html5/beta-distributions-1.png",
    "last_modified": "2021-03-09T21:00:38+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 652
  },
  {
    "path": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/",
    "title": "rpihkal: Combine ggplots with patchwork",
    "description": "How to combine arbitrary ggplots",
    "author": [
      {
        "name": "Matti Vuorre",
        "url": "https://vuorre.netlify.com"
      }
    ],
    "date": "2018-12-13",
    "categories": [
      "data science",
      "visualization",
      "ggplot2",
      "R",
      "rpihkal"
    ],
    "contents": "\n\nContents\nFacetting figures into small multiples\nCombining arbitrary ggplots\nPatchwork\nSupport this work\nSoftware used\n\n\nggplot2 is the best R package for data visualization, and has powerful features for “facetting” plots into small multiples based on categorical variables.\nFacetting figures into small multiples\nThis “facetting” is useful for showing the same figure, e.g. a bivariate relationship, at multiple levels of some other variable\n\n\nlibrary(tidyverse)\nggplot(mtcars, aes(mpg, disp)) +\n  geom_point() +\n  facet_wrap(\"cyl\")\n\n\n\n\nBut if you would like to get a figure that consists of multiple panels of unrelated plots—with different variables on the X and Y axes, potentially from different data sources—things become more complicated.\nCombining arbitrary ggplots\nSay you have these three figures\n\n\np <- ggplot(mtcars)\n  \na <- p +\n  aes(mpg, disp, col = as.factor(vs)) +\n  geom_smooth(se = F) +\n  geom_point()\n\nb <- p + \n  aes(disp, gear, group = gear) +\n  ggstance::geom_boxploth()\n\nc <- p +\n  aes(hp) +\n  stat_density(geom = \"area\") +\n  coord_cartesian(expand = 0)\n\n\n\nHow would you go about combining them? There are a few options, such as grid.arrange() in the gridExtra package, and plot_grid() in the cowplot package. Today, I’ll point out a newer package that introduces a whole new syntax for combining together, patchwork.\nPatchwork\npatchwork is not yet on CRAN, so install it from GitHub:\n\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"thomasp85/patchwork\")\n\n\n\nOnce you load the package, you can add ggplots together by adding them with +:\n\n\nlibrary(patchwork)\na + b + c\n\n\n\n\nBasically, you can add ggplots together as if they were geoms inside a single ggplot. However, there’s more. | specifies side-by-side addition\n\n\na | c\n\n\n\n\nAnd / is for adding plots under the previous plot\n\n\nb / c\n\n\n\n\nThese operators can be used to flexibly compose figures from multiple components, using parentheses to group plots and +, |, and / to add the groups together\n\n\n(a | b) / c\n\n\n\n\nUse plot_annotation() to add tags, and & to pass theme elements to all plot elements in a composition\n\n\n(a | b) / c + \n  plot_annotation(tag_levels = \"A\") & \n  theme(legend.position = \"none\")\n\n\n\n\nFigure 1: Tweak this a little bit and throw it in a manuscript.\n\n\n\nThere are many more examples on patchwork’s GitHub page. I’ve found this package more useful in composing figures out of multiple plots than its alternatives, mainly because of the concise but powerful syntax.\n\nSupport this work\n\nSoftware used\nThe following software packages were used: R [Version 4.0.3; R Core Team (2020)] and the R-packages dplyr [Version 1.0.4; Wickham et al. (2021)], forcats [Version 0.5.1; Wickham (2021a)], ggplot2 [Version 3.3.3; Wickham (2016)], knitr [Version 1.31; Xie (2015)], patchwork [Version 1.1.1; Pedersen (2020)], purrr [Version 0.3.4; Henry and Wickham (2020)], readr [Version 1.4.0; Wickham and Hester (2020)], stringr [Version 1.4.0; Wickham (2019)], tibble [Version 3.1.0; Müller and Wickham (2021)], tidyr [Version 1.1.3; Wickham (2021b)], and tidyverse [Version 1.3.0; Wickham et al. (2019)].\n\n\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nMüller, Kirill, and Hadley Wickham. 2021. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nPedersen, Thomas Lin. 2020. Patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2021a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2021b. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nXie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\n\n\n",
    "preview": "posts/2018-12-13-rpihkal-combine-ggplots-with-patchwork/2018-12-13-rpihkal-combine-ggplots-with-patchwork_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2021-03-09T21:03:21+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2018-12-12-rpihkal-stop-pasting-and-start-gluing/",
    "title": "rpihkal: Glue your strings together",
    "description": "Use the glue R package to join strings.",
    "author": [
      {
        "name": "Matti Vuorre",
        "url": "https://vuorre.netlify.com"
      }
    ],
    "date": "2018-12-12",
    "categories": [
      "data science",
      "R",
      "rpihkal"
    ],
    "contents": "\n\nContents\nPaste\nGlue\nGlue with data frames\n\nAppendix: papaja\nSupport this work\nSoftware used\n\n\nWe’ve all been there; writing manuscripts with R Markdown and dreaming of easy in-text code bits for reproducible reporting. Say you’ve fit a regression model to your data, and would then like to report the model’s parameters in your text, without writing the values in the text. (If the data or model changes, you’d need to re-type the values again.)\nFor example, you can print this model summary easily in the R console:\n\n\nfit <- lm(mpg ~ disp, data = mtcars)\nsummary(fit)\n\n\n\nCall:\nlm(formula = mpg ~ disp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8922 -2.2022 -0.9631  1.6272  7.2305 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 29.599855   1.229720  24.070  < 2e-16 ***\ndisp        -0.041215   0.004712  -8.747 9.38e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.251 on 30 degrees of freedom\nMultiple R-squared:  0.7183,    Adjusted R-squared:  0.709 \nF-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10\n\nAnd to cite those values in the text body of your manuscript, you can write the text in R Markdown like this:\n\nThe model intercept was `r round(coef(fit)[1], 2)`, great.\n\nWhich would show up in your manuscript like this:\nThe model intercept was 29.6, great.\nPaste\nHowever, when you want to present more information, such as the parameter estimate with its standard error, you will have to paste() those strings together:\n\n\n(x <- round(summary(fit)$coefficients, 3))\n\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   29.600      1.230  24.070        0\ndisp          -0.041      0.005  -8.747        0\n\nintercept <- paste(\"b = \", x[1, 1], \", SE = \", x[1, 2], sep = \"\")\n\n\n\nYou can then just cite the intercept object in your text body:\n\nThe model intercept was very very significant (`r intercept`).\n\nWhich would render in your PDF or word document as:\nThe model intercept was very very significant (b = 29.6, SE = 1.23).\npaste() is a base R function, and as such very robust and reproducible–all R installations will have it. However, as such it has a fairly terrible syntax where you have to quote strings, separate strings and variables with commas, etc. This task is made much easier with glue().\nGlue\nglue is a small R package that allows you to join strings together in a neat, pythonific way. It replaces the need for quoting and separating arguments in paste(), by asking you to wrap variables in curly braces. Here’s how to do the above pasting with glue:\n\n\nlibrary(glue)\nintercept <- glue(\"b = {x[1, 1]}, SE = {x[1, 2]}\")\n\n\n\nWhich gives you the same string as the much messier paste() approach: b = 29.6, SE = 1.23\nGlue with data frames\nGlue has other neat (more advanced) features, such as gluing variables row-by-row in a data frame:\n\n\nlibrary(dplyr)\nas.data.frame(x) %>% \n  glue_data(\n    \"{rownames(.)}'s point estimate was {Estimate}, with an SE of {`Std. Error`}.\"\n  )\n\n\n(Intercept)'s point estimate was 29.6, with an SE of 1.23.\ndisp's point estimate was -0.041, with an SE of 0.005.\n\nAppendix: papaja\nFor some models (like our simple linear model example here), the papaja R package (which deserves its own rpihkal post!) has very useful shortcuts\n\n\nlibrary(papaja)\nintercept <- apa_print(fit)$estimate$Intercept\n\n\n\nIf you now cite intercept in the text body of your manuscript, it renders into \\(\\LaTeX\\) (which is interpreted nicely if you are outputting PDF or Word documents; here on this website it looks odd):\n\nThe model intercept was rather significant (`r intercept`).\n\nThe model intercept was rather significant (\\(b = 29.60\\), 95% CI \\([27.09\\), \\(32.11]\\)).\nRead more about glue at https://glue.tidyverse.org/.\n\nSupport this work\n\nSoftware used\nThe following software packages were used: R [Version 4.0.3; R Core Team (2020)] and the R-packages dplyr [Version 1.0.4; Wickham et al. (2021)], forcats [Version 0.5.1; Wickham (2021a)], ggplot2 [Version 3.3.3; Wickham (2016)], glue [Version 1.4.2; Hester (2020)], knitr [Version 1.31; Xie (2015)], papaja [Version 0.1.0.9997; Aust and Barth (2020)], purrr [Version 0.3.4; Henry and Wickham (2020)], readr [Version 1.4.0; Wickham and Hester (2020)], stringr [Version 1.4.0; Wickham (2019)], tibble [Version 3.1.0; Müller and Wickham (2021)], tidyr [Version 1.1.3; Wickham (2021b)], and tidyverse [Version 1.3.0; Wickham et al. (2019)].\n\n\n\n\nAust, Frederik, and Marius Barth. 2020. papaja: Create APA Manuscripts with R Markdown. https://github.com/crsh/papaja.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nHester, Jim. 2020. Glue: Interpreted String Literals. https://CRAN.R-project.org/package=glue.\n\n\nMüller, Kirill, and Hadley Wickham. 2021. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2021a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2021b. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nXie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-09T21:02:56+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/",
    "title": "Bayesian Estimation of Signal Detection Models",
    "description": "Signal Detection Theory (SDT) is a popular theoretical framework for modeling memory and perception. Calculating point estimates of equal variance Gaussian SDT parameters is easy using widely known formulas. More complex SDT models, such as the unequal variance SDT model, require more complicated modeling techniques. These models can be estimated using Bayesian (nonlinear and/or hierarchical) regression methods, which are illustrated here.",
    "author": [
      {
        "name": "Matti Vuorre",
        "url": "https://vuorre.netlify.com"
      }
    ],
    "date": "2017-10-09",
    "categories": [
      "psychology",
      "statistics",
      "tutorial",
      "R",
      "brms"
    ],
    "contents": "\n\nContents\nSignal Detection Theory\nExample data\n\nEqual Variance Gaussian SDT Model\nCalculate EVSDT parameters’ point estimates\nEstimate EVSDT model with a GLM\nEstimate EVSDT with a nonlinear model\nInterim discussion\nFitting one subject’s EVSDT model with different methods\nPrior distribution\n\n\nEVSDT for multiple participants\nPopulation-level EVSDT Model\nEstimation by summarizing subjects’ point estimates\nEstimation with a hierarchical model (GLMM)\nIncluding predictors\nEstimation with a GLMM (nonlinear syntax)\n\nInterim discussion\n\nUnequal variance Gaussian SDT model\nExample data: Rating task\nEVSDT: one subject’s rating responses\nUVSDT: one subject’s rating responses\n\nUVSDT for multiple participants\nExample data set\nModel syntax\nPrior distributions\nEstimate and summarise parameters\nHeterogeneity parameters\n\n\nConclusion\nSupport this work\nSoftware used\n\n\nSignal Detection Theory\nSignal Detection Theory (SDT) is a common framework for modeling memory and perception. Calculating point estimates of equal variance Gaussian SDT parameters is easy using widely known formulas. More complex SDT models, such as the unequal variance SDT model, require more complicated modeling techniques. These models can be estimated using Bayesian (nonlinear and/or hierarchical) regression methods, which are sometimes difficult to implement in practice. In this tutorial, I describe how to estimate equal and unequal variance Gaussian SDT models as Generalized Linear Models for single participants, and for multiple participants simultaneously using hierarchical Bayesian models (or Generalized Linear Mixed Models).\nConsider a recognition memory experiment where participants are shown a series of images, some of which are new (participant has not seen before) and some of which are old (participant has seen before). Participants answer, for each item, whether they think they have seen the item before (“old!” response) or not (“new!” response). SDT models allow modeling participants’ sensitivity—how well they can distinguish new and old images—and response criterion—their tendency of bias to respond “old!”—separately, and can therefore be enormously useful in modeling the participants’ memory processes. This similar logic applies to e.g. perception, where SDT was initially introduced in.\nThe conceptual basis of SDT models is that on each trial, when a stimulus is presented, participants experience some inner “familiarity” (or memory strength) signal, which is hidden from the experimenter, or latent. The participants then decide, based on this familiarity signal, whether they have encountered the current stimulus stimulus previously (“old!”) or not (“new!”). I assume that readers are at least somewhat familiar with the basics of SDT, and will not discuss the underlying theory further. A classic introduction to the topic is Macmillan and Creelman (2005).\nExample data\nWe move on to examining a practical example using the R statistical programming environment (R Core Team 2017). The following R packages were used in this tutorial:\n\n\nlibrary(knitr)\nlibrary(scales)\nlibrary(bayesplot)\nlibrary(ggridges)\nlibrary(sdtalt)  # devtools::install_github(\"cran/sdtalt\") (not on CRAN)\nlibrary(brms)\nlibrary(tidyverse)\n\n\n\nThe example data is called confcontr, and is provided as a data frame in the sdtalt package (Wright 2011a): “These are the data from the control group in Skagerberg and Wright’s study of memory conformity. Basically, this is the simplest old/new recognition memory design.” (Skagerberg and Wright 2008).\n\n\ndata(confcontr)\n\n\n\n\nTable 1: Example recognition memory data\nsubno\nsayold\nisold\n53\n1\n0\n53\n1\n1\n53\n1\n1\n53\n1\n1\n53\n1\n0\n53\n1\n1\n\nEqual Variance Gaussian SDT Model\nWe consider the most common SDT model, that assumes the participants’ distributions of familiarity are two Gaussian distributions with equal variances, but possibly different means (i.e. previously seen items elicit a stronger familiarity signal, on average). This model is known as the EVSDT (equal variance SDT) model.\nWe estimate the model’s parameters for a single participant using three methods: “Manual” calculation of the point estimates using easy formulas translated to R code; estimating the model using a Bayesian Generalized Linear Model; and estimating the model using a Bayesian nonlinear model.\nCalculate EVSDT parameters’ point estimates\nWe begin by calculating the maximum likelihood estimates of the EVSDT parameters, separately for each participant in the data set. Before doing so, I note that this data processing is only required for manual calculation of the point estimates; the modeling methods described below take the raw data and therefore don’t require this step.\nFirst, we’ll compute for each trial whether the participant’s response was a hit, false alarm, correct rejection, or a miss. We’ll do this by creating a new variable, type:\n\n\nsdt <- confcontr %>%\n  mutate(\n    type = \"hit\",\n    type = ifelse(isold == 1 & sayold == 0, \"miss\", type),\n    type = ifelse(isold == 0 & sayold == 0, \"cr\", type), # Correct rejection\n    type = ifelse(isold == 0 & sayold == 1, \"fa\", type) # False alarm\n  )\n\n\n\nThen we can simply count the numbers of these four types of trials for each participant, and put the counts on one row per participant.\n\n\nsdt <- sdt %>%\n  group_by(subno, type) %>%\n  summarise(count = n()) %>%\n  spread(type, count) # Format data to one row per person\n\n\n\nFor a single subject, d’ can be calculated as the difference of the standardized hit and false alarm rates (Stanislaw and Todorov 1999):\n\\[d' = \\Phi^{-1}(HR) - \\Phi^{-1}(FAR)\\]\n\\(\\Phi\\) is the cumulative normal density function, and is used to convert z scores into probabilities. Its inverse, \\(\\Phi^{-1}\\), converts a proportion (such as a hit rate or false alarm rate) into a z score. From here on, I refer to standardized hit and false alarm rates as zHR and zFAR, respectively. The response criterion c is given by the negative standardized false alarm rate -zFAR (DeCarlo 1998).\nWe can use R’s proportion to z-score function (\\(\\Phi^{-1}\\)), qnorm(), to calculate each participant’s d’ and c from the counts of hits, false alarms, misses and correct rejections:\n\n\nsdt <- sdt %>%\n  mutate(\n    zhr = qnorm(hit / (hit + miss)),\n    zfa = qnorm(fa / (fa + cr)),\n    dprime = zhr - zfa,\n    crit = -zfa\n  )\n\n\nTable 2: Point estimates of EVSDT parameters\nsubno\ncr\nfa\nhit\nmiss\nzhr\nzfa\ndprime\ncrit\n53\n33\n20\n25\n22\n0.08\n-0.31\n0.39\n0.31\n54\n39\n14\n28\n19\n0.24\n-0.63\n0.87\n0.63\n55\n36\n17\n31\n16\n0.41\n-0.47\n0.88\n0.47\n56\n43\n10\n38\n9\n0.87\n-0.88\n1.76\n0.88\n57\n35\n18\n29\n18\n0.30\n-0.41\n0.71\n0.41\n58\n41\n12\n30\n17\n0.35\n-0.75\n1.10\n0.75\n\nThis data frame now has point estimates of every participant’s d’ and c. The implied EVSDT model for participant 53 is shown in Figure 1.\n\n\n\nFigure 1: The equal variance Gaussian signal detection model for the first participant in the data, based on manual calculation of the parameter’s point estimates. The two distributions are the noise distribution (dashed) and the signal distribution (solid); the dotted vertical line represents the response criterion. d’ is the distance between the peaks of the two distributions.\n\n\n\nEstimate EVSDT model with a GLM\nGeneralized Linear Models (GLM) are a powerful class of regression models that allow modeling binary outcomes, such as our “old!” / “new!” responses. In confcontr, each row (trial) can have one of two responses, “old!” (sayold = 1) or “new!” (sayold = 0). We use GLM to regress these responses on the stimulus type: On each trial, the to-be-judged stimulus can be either new (isold = 0) or old (isold = 1).\nIn a GLM of binary outcomes, we assume that the outcomes are Bernoulli distributed (binomial with 1 trial), with probability \\(p_i\\) that \\(y_i = 1\\).\n\\[y_i \\sim Bernoulli(p_i)\\]\nBecause probabilities have upper and lower bounds at 1 and 0, and we wish to use a linear model (generalized linear model) of the p parameter, we don’t model p with a linear model. Instead, we map p to a “linear predictor” \\(\\eta\\) with a link function, and model \\(\\eta\\) with a linear regression model. If this link function is probit, we have a “probit GLM”:\n\nYou are probably familiar with logistic regression models, which are just another binary GLM, but with the logistic link function!\n\\[p_i = \\Phi(\\eta_i)\\]\n\\(\\Phi\\) is again the cumulative normal density function and maps z scores to probabilities. We then model \\(\\eta\\) on an intercept and a slope:\n\\[\\eta_i = \\beta_0 + \\beta_1\\mbox{isold}_i\\]\nGiven this parameterization, the intercept of the model (\\(\\beta_0\\)) is going to be the standardized false alarm rate (probability of saying 1 when predictor is 0), which we take as our criterion c. The slope of the model is the increase in the probability of saying 1 when the predictor is 1, in z-scores, which is another way of saying d’. Therefore, \\(c = -zHR = -\\beta_0\\), and \\(d' = \\beta_1\\).\nThe connection between SDT models and GLM is discussed in detail by DeCarlo (1998). Two immediate benefits of thinking about SDT models in a GLM framework is that we can now easily include predictors on c and d’, and estimate SDT models with varying coefficients using hierarchical modeling methods (DeCarlo 2010; Rouder and Lu 2005). This latter point means that we can easily fit the models for multiple participants (and items!) simultaneously, while at the same time pooling information across participants (and items). We will return to this point below.\nBecause we wrote the SDT model as a GLM, we have a variety of software options for estimating the model. For this simple model, you could just use base R’s glm(). Here, we use the Bayesian regression modeling R package brms (Bürkner 2017b; Stan Development Team 2016a), because its model formula syntax extends seamlessly to more complicated models that we will discuss later. We can estimate the GLM with brms’s brm() function, by providing as arguments a model formula in brms syntax (identical to base R model syntax for simple models), an outcome distribution with a link function, and a data frame.\nbrms’s model syntax uses variable names from the data. We regress the binary sayold responses on the binary isold predictor with the following formula: sayold ~ isold. The distribution of the outcomes is specified with family argument. To specify the bernoulli distribution with a probit link function, we use family = bernoulli(link=\"probit\"). We will only model the first participant’s data (number 53), and therefore specify the data with data = filter(confcontr, subno==53).\nThe brm() function also allows specifying prior distributions on the parameters, but for this introductory discussion we omit discussion of priors. In addition, to run multiple MCMC chains (Kruschke 2014; van Ravenzwaaij, Cassey, and Brown 2016) in parallel, we set the cores argument to 4 (this makes the model estimation faster). Finally, we also specify file, to save the model to a file so that we don’t have to re-estimate the model whenever we restart R.\nPutting these pieces together, we estimate the SDT model as a probit GLM, using data stored in confcontr, for subject 53 only, with the following function:\n\n\nevsdt_1 <- brm(\n  sayold ~ isold,\n  family = bernoulli(link = \"probit\"),\n  data = filter(confcontr, subno == 53),\n  cores = 4,\n  file = \"sdtmodel1-1\"\n)\n\n\n\nThe estimated model is saved in evsdt_1, whose summary() method returns a numerical summary of the estimated parameters along with some information and diagnostics about the model:\n\n\nsummary(evsdt_1)\n\n\n Family: bernoulli \n  Links: mu = probit \nFormula: sayold ~ isold \n   Data: filter(confcontr, subno == 53) (Number of observations: 100) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.31      0.18    -0.68     0.04 1.00     3487     2580\nisold         0.39      0.25    -0.09     0.88 1.00     3634     2900\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe regression parameters (Intercept (recall, \\(c = -\\beta_0\\)) and isold (\\(d' = \\beta_1\\))) are described in the “Population-Level Effects” table in the above output. Estimate reports the posterior means, which are comparable to maximum likelihood point estimates, and Est.Error reports the posterior standard deviations, which are comparable to standard errors. The next two columns report the parameter’s 95% Credible Intervals (CIs). The estimated parameters’ means match the point estimates we calculated by hand (see table above.)\nIn fact, the posterior modes will exactly correspond to the maximum likelihood estimates, if we use uniform priors. The posterior density of d’ and c, for participant 53, is illustrated in Figure 2: The maximum likelihood estimate is spot on the highest peak of the posterior density.\n\n\n\nFigure 2: The (approximate) joint posterior density of subject 53’s SDT parameters. Lighter yellow colors indicate higher posterior density. The red dot indicates the ‘manually’ calculated MLE point estimate of d’.\n\n\n\nFigure 2 raises some interesting questions: What happens if we ignore the uncertainty in the estimated parameters (the colorful cloud of decreasing plausibility around the peak)? The answer is that not much happens for inference about averages by ignoring the subject-specific parameters’ uncertainty, if the design is balanced across participants. But what will happen if we use the point estimates as predictors in some other regression, while ignoring their uncertainty? What are the implications of having very uncertain estimates? Should we trust the mode?\nIn any case, I hope the above has illustrated that the equal variance Gaussian SDT parameters are easy to obtain within the GLM framework. Next, we describe how to estimate the SDT model using brms’ nonlinear modeling syntax.\nEstimate EVSDT with a nonlinear model\nHere, we write the EVSDT model in a similar way as the GLM above, but simply flip the criterion and d’. To do that we need to use brms’ nonlinear modelling syntax. This parameterization will give c directly, without the need to flip the estimated parameter value. Although conceptually similar to above, and not necessarily useful by itself, it might be useful to fit this small variation of the above GLM to get familiar with brms’ nonlinear modeling syntax. We write the model as follows (DeCarlo 1998):\n\\[p_i = \\Phi(d'\\mbox{isold}_i - c)\\]\nThis model gives us direct estimates of c and d’. Writing and estimating nonlinear models can be considerably more involved than fitting GLMs. Accordingly, the code below is a bit more complicated. The key point here is, however, that using brms, we can estimate models that may be nonlinear without deviating too far from the basic formula syntax.\nFirst, we’ll specify the model using the bf() function:\n\n\nm2 <- bf(\n  sayold ~ Phi(dprime * isold - c),\n  dprime ~ 1, c ~ 1,\n  nl = TRUE\n)\n\n\n\nLet’s walk through this code line by line. On the first line, we specify the model of sayold responses. Recall that we are modeling the responses as Bernoulli distributed (this will be specified as an argument to the estimation function, below). Therefore, the right-hand side of the first line (after ~) is a model of the probability parameter (\\(p_i\\)) of the Bernoulli distribution.\nThe two unknown parameters in the model, d’ and c, are estimated from data, as indicated by the second line (i.e. dprime ~ 1). The third line is required to tell brms that the model is nonlinear. To further understand how to write models with brms’ nonlinear modeling syntax, see (vignette(\"brms_nonlinear\", package = \"brms\")) (or here).\nBecause the parameters of nonlinear models can be more difficult to estimate, brms requires the user to set priors when nl = TRUE. We set somewhat arbitrary priors on dprime and c (the scale parameter is standard deviation, not variance):\n\n\nPriors <- c(\n  prior(normal(.5, 3), nlpar = \"dprime\"),\n  prior(normal(0, 1.5), nlpar = \"c\")\n)\n\n\n\nAfter specifying the model and priors, fitting the model is done again using brm() with only a few adjustments: because we specified the link function inside bf() (the Phi() function), we should explicitly set link=\"identity\" in the family argument. Because nonlinear models are trickier to estimate, we also adjust the underlying Stan sampler’s adapt_delta parameter (this will make the MCMC a little slower but will return less noisy results).\n\n\nevsdt_2 <- brm(\n  m2,\n  family = bernoulli(link = \"identity\"),\n  data = filter(confcontr, subno == 53),\n  prior = Priors,\n  control = list(adapt_delta = .99),\n  cores = 4,\n  file = \"sdtmodel1-2\"\n)\n\n\n\nNotice that we now entered m2 as the first argument, whereas with the first model, we simply wrote the formula inside the brm() function. These two ways are equivalent, but because this model is more complicated, I saved it into a variable as a separate line of code.\nWe can then compare the two models’ estimated parameters. Recall that the latter model directly reports the standardized false alarm rate (c).\n\n\nsummary(evsdt_1)\n\n\n Family: bernoulli \n  Links: mu = probit \nFormula: sayold ~ isold \n   Data: filter(confcontr, subno == 53) (Number of observations: 100) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.31      0.18    -0.68     0.04 1.00     3487     2580\nisold         0.39      0.25    -0.09     0.88 1.00     3634     2900\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(evsdt_2)\n\n\n Family: bernoulli \n  Links: mu = identity \nFormula: sayold ~ Phi(dprime * isold - c) \n         dprime ~ 1\n         c ~ 1\n   Data: filter(confcontr, subno == 53) (Number of observations: 100) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndprime_Intercept     0.39      0.25    -0.10     0.88 1.00     1202     1551\nc_Intercept          0.31      0.18    -0.04     0.65 1.00     1125     1595\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe results are very similar, but note that priors were included only in the nonlinear syntax model. The only real difference is that the MCMC algorithm explored evsdt_2’s posterior less efficiently, as shown by the smaller effective sample sizes (..._ESS) for both parameters. This means that the random draws from the posterior distribution, for evsdt_2, have greater autocorrelation, and therefore we should possibly draw more samples for more accurate inference. The posterior distributions obtained with the 2 methods are shown in Figure 3.\n\n\n\nFigure 3: Top row: The (approximate) joint posterior density of subject 53’s SDT parameters, estimated with the GL model and the nonlinear model. Lighter yellow colors indicate higher posterior density. The red dot indicates the sample mean d’ that was calculated ‘manually.’ Bottom row: The marginal posterior densities of c and dprime from GLM (red) and nonlinear (blue) models.\n\n\n\nThere is little benefit in using the second, “nonlinear” parameterization of EVSDT in this case. However, it is useful to study this simpler case to make it easier to understand how to fit more complicated nonlinear models with brms.\nInterim discussion\nFitting one subject’s EVSDT model with different methods\nWe have now estimated the equal variance Gaussian SDT model’s parameters for one subject’s data using three methods: Calculating point estimates manually, with a probit GLM, and with a probit model using brms’ nonlinear modeling syntax. The main difference between these methods, so far, is that the modeling methods provide estimates of uncertainty in the parameters, whereas the manual calculation does not. This point leads us directly to hierarchical models (Rouder and Lu 2005; Rouder et al. 2007), which we discuss next.\nHowever, there are other, perhaps more subtle, benefits of using a regression model framework for estimating SDT models. There is something to be said, for example, about the fact that the models take the raw data as input. ‘Manual’ calculation involves, well, manual computation of values, which may be more error prone than using raw data. This is especially clear if the modeling methods are straightforward to apply: I hope to have illustrated that with R and brms (Bürkner 2017b), Bayesian modeling methods are easy to apply and accessible to a wide audience.\nMoving to a modeling framework will also allow us to include multiple sources of variation, such as heterogeneity across items and participants, through crossed “random” effects (Rouder et al. 2007), and covariates that we think might affect the SDT parameters. By changing the link function, we can also easily use other distributions, such as logistic, to represent the signal and noise distributions (DeCarlo 1998, 2010).\nPrior distribution\nFinally, priors. Newcomers to the Bayesian modeling framework might object to the use of prior distributions, and think that they are unduly biasing the results. However, moderately informative priors usually have far less of an influence on inference than newcomers might assume. Above, we specified the GLM with practically no prior information; if you are reluctant to include existing knowledge into your model, feel free to leave it out. Things are, unfortunately, a little more complicated with the nonlinear modeling functions: The posterior geometry might be funky (technical term), in which case the priors could mainly serve to nudge the posterior samples to be drawn from sensible parameter values.\nFurther, priors can be especially useful in estimating SDT models: If participants’ hit or false alarm rates are 0 or 1–a fairly common scenario–mild prior information can be used in a principled manner to release the estimated quantities from the hostile captivity of the boundary values. Prior literature has discussed various corrections to 0 and 1 rates (Stanislaw and Todorov 1999). However, Bayesian priors can take care of these edge cases in a more principled manner.\nEVSDT for multiple participants\nAbove, we obtained parameter estimates of the EVSDT model for a single subject using three methods: Manual calculation of point estimates (Stanislaw and Todorov 1999), estimating the model as a GLM (Generalized Linear Model; DeCarlo (1998)), and estimating the model as a GLM using brms’ nonlinear modeling syntax (Bürkner 2017b).\nHowever, researchers are usually not as interested in the specific subjects that happened to participate in their experiment, as they are in the population of potential subjects. Therefore, we are unsatisfied with parameters which describe only the subjects that happened to participate in our study: The final statistical model should have parameters that estimate features of the population of interest.\nBroadly, there are two methods for obtaining these “population level” parameters. By far the most popular method is to summarise the manually calculated subject-specific point estimates of d’ and c with their sample means and standard deviations. From these, we can calculate standard errors, t-tests, confidence intervals, etc. Another method–which I hope to motivate here–is to build a bigger model that estimates subject-specific and population-level parameters simultaneously. We call this latter method “hierarchical” or “multilevel” modeling (Gelman and Hill 2007; Rouder and Lu 2005). In this section, I show how to obtain population-level EVSDT parameters with these two methods, using the R programming language and the brms R package (R Core Team 2017; Bürkner 2017b).\nPopulation-level EVSDT Model\nWe now use these data to estimate the population-level EVSDT parameters using two methods: Manual calculation and hierarchical modeling. For hierarchical modeling, I provide R & brms code to estimate the model as a Generalized Linear Mixed Model (GLMM). I also show how to estimate the GLMM with brms’ nonlinear modeling syntax.\nEstimation by summarizing subjects’ point estimates\nAbove we calculated d’ and c for every participant in the sample:\n\nTable 3: Sample participants’ SDT parameters\nsubno\ncr\nfa\nhit\nmiss\nzhr\nzfa\ndprime\ncrit\n53\n33\n20\n25\n22\n0.08\n-0.31\n0.39\n0.31\n54\n39\n14\n28\n19\n0.24\n-0.63\n0.87\n0.63\n55\n36\n17\n31\n16\n0.41\n-0.47\n0.88\n0.47\n56\n43\n10\n38\n9\n0.87\n-0.88\n1.76\n0.88\n57\n35\n18\n29\n18\n0.30\n-0.41\n0.71\n0.41\n58\n41\n12\n30\n17\n0.35\n-0.75\n1.10\n0.75\n\nWe can therefore calculate sample means and standard errors for both parameters using these individual-specific values. Here’s one way to do it:\n\n\nsdt_sum <- select(sdt, subno, dprime, crit) %>% # Select these variables only\n  gather(parameter, value, -subno) %>% # Convert data to long format\n  group_by(parameter) %>% # Prepare to summarise on these grouping variables\n  # Calculate summary statistics for grouping variables\n  summarise(n = n(), mu = mean(value), sd = sd(value), se = sd / sqrt(n))\n\n\nTable 4: Average EVSDT parameters\nparameter\nn\nmu\nsd\nse\ncrit\n31\n0.67\n0.33\n0.06\ndprime\n31\n1.09\n0.50\n0.09\n\nThe sample means (mu) are estimates of the population means, and the sample standard deviations (sd) divided by \\(\\sqrt{N subjects}\\) are estimated standard deviations of the respective sampling distributions: the standard errors (se). Because the standard deviations of the sampling distributions are unknown and therefore estimated from the data, researchers almost always substitute the Gaussian sampling distribution with a Student’s t-distribution to obtain p-values and confidence intervals (i.e. we run t-tests, not z-tests.)\nNote that this method involves calculating point estimates of unknown parameters (the subject-specifc parameters), and then summarizing these parameters with additional models. In other words, we first fit N models with P parameters each (N = number of subjects, P = 2 parameters), and then P more models to summarise the subject-specific models.\nNext, we’ll use hierarchical regression1 methods to obtain subject-specific and population-level parameters in one single step.\nEstimation with a hierarchical model (GLMM)\nWe can estimate the EVSDT model’s parameters for every subject and the population average in one step using a Generalized Linear Mixed Model (GLMM). Gelman and Hill (2007) and McElreath (2016) are good general introductions to hierarchical models. Rouder and Lu (2005) and Rouder et al. (2007) discuss hierarchical modeling in the context of signal detection theory.\nThis model is very much like the GLM discussed in Part 1, but now the subject-specific d’s and cs are modeled as draws from a multivariate normal distribution, whose (“hyper”)parameters describe the population-level parameters. We subscript subjects’ parameters with j, rows in data with i, and write the model as:\n\\[y_{ij} \\sim Bernoulli(p_{ij})\\] \\[\\Phi(p_{ij}) = \\beta_{0j} + \\beta_{1j}\\mbox{isold}_{ij}\\]\nThe outcomes \\(y_{ij}\\) are 0 if participant j responded “new!” on trial i, 1 if they responded “old!” The probability of the “old!” response for row i for subject j is \\(p_{ij}\\). We then write a linear model on the probits (z-scores; \\(\\Phi\\), “Phi”) of ps. The subject-specific intercepts (recall, \\(\\beta_0\\) = -zFAR) and slopes (\\(\\beta_1\\) = d’) are described by multivariate normal with means and a covariance matrix for the parameters.\n\\[\n\\left[\\begin{array}{c}\n\\beta_{0j} \\\\ \\beta_{1j}\n\\end{array}\\right] \n\\sim MVN(\n\\left[\\begin{array}{c}\n\\mu_{0} \\\\ \\mu_{1}\n\\end{array}\\right],\n\\Sigma\n)\n\\]\nThe means \\(\\mu_0\\) and \\(\\mu_1\\), i.e. the population-level parameters, can be interpreted as parameters “for the average person” (Bolger and Laurenceau 2013). The covariance matrix \\(\\Sigma\\) contains the subject-specific parameters’ (co)variances, but I find it easier to discuss standard deviations (I call them \\(\\tau\\), “tau”) and correlations. The standard deviations describe the between-person heterogeneities in the population. The correlation term, in turn, describes the covariance of the d’s and cs: Are people with higher d’s more likely to have higher cs?\nThis model is therefore more informative than running multiple separate GLMs, because it models the covariances as well, answering important questions about heterogeneity in effects.\nThe brms syntax for this model is very similar to the one-subject model. We have five population-level parameters to estimate. The intercept and slope describe the means: In R and brms modeling syntax, an intercept is indicated with 1 (and can be omitted because it is automatically included, here I include it for clarity), and slope of a variable by including that variable’s name in the data. To include the two regression coefficients, we write sayold ~ 1 + isold.\nHowever, we also have three (co)variance parameters to estimate. To include subject-specific parameters (recall, subjects are indexed by subno variable in data d), and therefore the (co)variance parameters, we expand the formula to sayold ~ 1 + isold + (1 + isold | subno). The part in the parentheses describes subno specific intercepts (1) and slopes of isold. Otherwise, the call to brm() is the same as with the GLM in Part 1:\n\n\nevsdt_glmm <- brm(sayold ~ 1 + isold + (1 + isold | subno),\n  family = bernoulli(link = \"probit\"),\n  data = confcontr,\n  cores = 4,\n  file = \"sdtmodel2-1\"\n)\n\n\n\nLet’s take a look at the GLMM’s estimated parameters. First, direct your eyes to the “Population-Level Effects” table in the below output. These two parameters are the mean -criterion (Intercept, \\(\\mu_0\\)) and d’ (isold, \\(\\mu_1\\)). Recall that we are looking at numerical summaries of (random samples from) the parameters’ posterior distributions: Estimate is the posterior mean.\n\n\nsummary(evsdt_glmm)\n\n\n Family: bernoulli \n  Links: mu = probit \nFormula: sayold ~ 1 + isold + (1 + isold | subno) \n   Data: confcontr (Number of observations: 3100) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nGroup-Level Effects: \n~subno (Number of levels: 31) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            0.26      0.06     0.16     0.38 1.00     1379     2098\nsd(isold)                0.38      0.08     0.24     0.56 1.00     1058     2014\ncor(Intercept,isold)    -0.56      0.19    -0.84    -0.12 1.00     1041     1869\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.66      0.06    -0.78    -0.55 1.00     1632     2208\nisold         1.06      0.09     0.89     1.23 1.00     1565     1889\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nWe can then compare the Population-level mean parameters of this model to the sample summary statistics we calculated above. The posterior means map nicely to the calculated means, and the posterior standard deviations match the calculated standard errors.\nThese mean effects are visualized as a colored density in the left panel of Figure 4. However, the GLMM also returns estimates of the parameters’ (co)variation in the population. Notice that we also calculated the sample standard deviations, which also provide this information, but we have no estimates of uncertainty in those point estimates. The GLMM, on the other hand, provides full posterior distributions for these parameters.\nThe heterogeneity parameters are reported in the “Group-Level Effects”2 table, above. We find that the criteria are positively correlated with d’s (recall that Intercept = -c). The two standard deviations are visualized in the right panel of Figure 4.\n\n\n\nFigure 4: Left panel: The (approximate) joint posterior density of the average d’ and criterion. Lighter values indicate higher posterior probability. Right panel: The (approximate) joint posterior density of the standard deviations of d’s and criteria in the population. In both panels, the red dot indicates the ‘manually’ calculated sample statistics.\n\n\n\nIt is evident in Figure 4 that the sample means approximately match the posterior mode, but less so for the sample standard deviations, which are far from the peak of the standard deviations’ posterior distribution. By ignoring the uncertainty in the subject-specific parameters, the ‘manual calculation’ method has over-estimated the heterogeneity of d’s and cs in the population, in comparison to the GLMM which takes the subject-specific parameters’ uncertainty into account.\nThis idea has further implications, revealed by investigating the two methods’ estimates of the subject-specific parameters. Recall that the manual calculation method involved estimating (the point estimates of) a separate model for each participant. A hierarchical model considers all participants’ data simultaneously, and the estimates are allowed to inform each other via the shared prior distribution (right hand side of the equation repeated from above):\n\\[\n\\left[\\begin{array}{c}\n\\beta_{0j} \\\\ \\beta_{1j}\n\\end{array}\\right] \n\\sim N(\n\\left[\\begin{array}{c}\n\\mu_{0} \\\\ \\mu_{1}\n\\end{array}\\right],\n\\Sigma\n)\n\\]\nThis “partial pooling” of information (Gelman and Hill 2007) is evident when we plot the GLMM’s subject-specific parameters in the same scatterplot with the N models method (calculating point estimates separately for everybody; Figure 5).\n\n\n\nFigure 5: Subject-specific d’s and criteria as given by the independent models (filled circles), and as estimated by the hierarchical model (empty circles). The hierarchical model shrinks the estimated parameters toward the overall mean parameters (red dot). This shrinkage is greater for more extreme parameter values: Each subject-specific parameter is a compromise between that subject’s data, and other subjects in the sample. As the data points per subject, or the heterogeneity between subjects, increases, this shrinkage will decrease. The hierarchical model essentially says ‘People are different, but not that different.’\n\n\n\nWe see that estimating the EVSDT model for many individuals simultaneously with a hierarchical model is both easy to fit and informative. Specifically, it is now easy to include predictors on the parameters, and answer questions about possible influences on d’ and c.\nIncluding predictors\nDo the EVSDT parameters differ between groups of people? How about between conditions, within people? To answer these questions, we would repeat the manual calculation of parameters as many times as needed, and then draw inference by “submitting” the subject-specific parameters to e.g. an ANOVA model. The GLMM approach affords a more straightforward solution to including predictors: We simply add parameters to the regression model.\nFor example, if there were two groups of participants, indexed by variable group in data, we could extend the brms GLMM syntax to (the ... is a placeholder for other arguments used above, I also dropped the 1 for clarity because they are implicitly included):\n\n\nbrm(sayold ~ isold * group + (isold | subno), ...)\n\n\n\nThis model would have two additional parameters: group would describe the difference in c between groups, and the interaction term isold:group would describe the difference in d’ between groups. If, on the other hand, we were interested in the effects of condition, a within-subject manipulation, we would write:\n\n\nbrm(sayold ~ isold * condition + (isold * condition | subno), ...)\n\n\n\nWith small changes, this syntax extends to “mixed” between- and within-subject designs.\nEstimation with a GLMM (nonlinear syntax)\nHere, I briefly describe fitting the above GLMM with brms’ nonlinear model syntax. The basic model is a straightforward reformulation of the single-subject case in Part 1 and the GLMM described above:\n\\[p_{ij} = \\Phi(d'_j\\mbox{isold}_{ij} - c_{j})\\]\nThe varying d-primes and criteria are modeled as multivariate normal, as with the GLMM. It turns out that this rather complex model is surprisingly easy to fit with brms. The formula is very similar to the single-subject nonlinear model but we tell bf() that the dprimes and criteria should have subject-specific parameters, as well as population-level parameters.\nAbove, with the GLMM, subject-specific effects were given by (1 + isold | subno). With the nonlinear modeling syntax, we specify varying effects across multiple parameters using |s| instead of | to tell brms that these parameters should be within one covariance matrix. This syntax gives us the “correlated random effects signal detection model” discussed in Rouder et al. (2007). Apart from the syntax, the model is the same as the GLMM above, but the sign of the intercept is flipped.\n\n\nglmm2 <- bf(sayold ~ Phi(dprime * isold - c),\n  dprime ~ 1 + (1 | s | subno),\n  c ~ 1 + (1 | s | subno),\n  nl = TRUE\n)\n\n\n\nThis time, we’ll set priors on the mean parameters and on the (co)variance parameters. Of note is the lkj(4) parameter which slightly regularizes the d’-criterion correlation toward zero (McElreath 2016; Stan Development Team 2016b).\n\n\nPriors <- c(\n  prior(normal(0, 3), nlpar = \"dprime\", lb = 0),\n  prior(normal(0, 3), nlpar = \"c\"),\n  prior(student_t(10, 0, 1), class = \"sd\", nlpar = \"dprime\"),\n  prior(student_t(10, 0, 1), class = \"sd\", nlpar = \"c\"),\n  prior(lkj(4), class = \"cor\")\n)\n\n\n\nWe fit the model as before, but adjust the control argument, and set inits to zero to improve sampling efficiency (thanks to Tom Wallis for this tip):\n\n\nevsdt_glmm2 <- brm(glmm2,\n  family = bernoulli(link = \"identity\"),\n  data = confcontr,\n  prior = Priors,\n  control = list(adapt_delta = .99),\n  cores = 4, inits = 0,\n  file = \"sdtmodel2-2\"\n)\n\n\n\nAlthough this model samples less efficiently than the first GLMM formulation, we (unsurprisingly) observe similar results.\n\n\nsummary(evsdt_glmm)\n\n\n Family: bernoulli \n  Links: mu = probit \nFormula: sayold ~ 1 + isold + (1 + isold | subno) \n   Data: confcontr (Number of observations: 3100) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nGroup-Level Effects: \n~subno (Number of levels: 31) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            0.26      0.06     0.16     0.38 1.00     1379     2098\nsd(isold)                0.38      0.08     0.24     0.56 1.00     1058     2014\ncor(Intercept,isold)    -0.56      0.19    -0.84    -0.12 1.00     1041     1869\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.66      0.06    -0.78    -0.55 1.00     1632     2208\nisold         1.06      0.09     0.89     1.23 1.00     1565     1889\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(evsdt_glmm2)\n\n\n Family: bernoulli \n  Links: mu = identity \nFormula: sayold ~ Phi(dprime * isold - c) \n         dprime ~ 1 + (1 | s | subno)\n         c ~ 1 + (1 | s | subno)\n   Data: confcontr (Number of observations: 3100) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nGroup-Level Effects: \n~subno (Number of levels: 31) \n                                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(dprime_Intercept)                  0.36      0.08     0.23     0.52 1.00     1687     2077\nsd(c_Intercept)                       0.24      0.05     0.15     0.35 1.00     1199     2077\ncor(dprime_Intercept,c_Intercept)     0.43      0.20    -0.01     0.75 1.00     1111     2041\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndprime_Intercept     1.06      0.08     0.90     1.22 1.00     1514     2148\nc_Intercept          0.66      0.06     0.55     0.77 1.00     1726     2400\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nFor technical reasons, each parameter in evsdt_glmm2 has a _Intercept suffix, but the results are the same across the two ways of writing this model.\nInterim discussion\nHierarchical modeling techniques have several advantages over traditional methods, such as (M)ANOVA, for modeling data with within-subject manipulations and repeated measures. For example, many models that previously required using parameters from subject-specific models as inputs to another model can be modeled within a single hierarchical model. Hierarchical models naturally account for unbalanced data, and allow incorporating continuous predictors and discrete outcomes. In the specific context of SDT, we observed that hierarchical models also estimate important parameters that describe possible between-person variability in parameters in the population of interest.\nFrom casual observation, it appears that hierarchical models are becoming more widely used. Many applied papers now analyze data using multilevel models, instead of rm-ANOVA, suggesting that there is demand for these models within applied research contexts. Conceptualizing more complex, possibly nonlinear models as hierarchical models should then afford a more unified framework for data analysis. Furthermore, by including parameters for between-person variability, these models allow researchers to quantify the extent to which their effects of interest vary and, possibly, whether these effects hold for everybody in the population.\nUnequal variance Gaussian SDT model\nNext, I extend the discussion to rating tasks to show how unequal variance Gaussian SDT (UVSDT) models can be estimated with with Bayesian methods, using R and the brms package (Bürkner 2017b; R Core Team 2017). As above, we first focus on estimating the model for a single participant, and then discuss hierarchical models for multiple participants.\nExample data: Rating task\nWe begin with a brief discussion of the rating task, with example data from Decarlo (2003). Above, we discussed signal detection experiments where the item was either old or new, and participants provided binary “old!” or “new!” responses. Here, we move to a slight modification of this task, where participants are allowed to express their certainty: On each trial, the presented item is still old or new, but participants now rate their confidence in whether the item was old or new. For example, and in the data below, participants can answer with numbers indicating their confidence that the item is old: 1 = Definitely new, …, 6 = Definitely old.\nOne interpretation of the resulting data is that participants set a number of criteria for the confidence ratings, such that greater evidence is required for 6-responses, than 4-responses, for example. That is, there will be different criteria for responding “definitely new,” “maybe new,” and so forth. However, the participant’s underlying discriminability should remain unaffected.\nThe example data is shown in a summarised form below (counts of responses for each confidence bin, for both old (isold = 1) and new trial types (Decarlo 2003)):\n\n\ndsum <- tibble(\n  isold = c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1),\n  y = c(1:6, 1:6),\n  count = c(174, 172, 104, 92, 41, 8, 46, 57, 66, 101, 154, 173)\n)\n\n\nTable 5: Example rating data from Decarlo (2003)\nisold\ny\ncount\n0\n1\n174\n0\n2\n172\n0\n3\n104\n0\n4\n92\n0\n5\n41\n0\n6\n8\n1\n1\n46\n1\n2\n57\n1\n3\n66\n1\n4\n101\n1\n5\n154\n1\n6\n173\n\nHowever, we don’t need to summarise data to counts (or cell means, or the like), but can instead work with raw responses, as provided by the experimental program. Working with such trial-level data is especially useful when we wish to include covariates. Here is the data in the raw trial-level format:\n\n\nd <- uncount(dsum, weights = count)\n\n\nTable 6: Example rating data in raw format from Decarlo (2003)\nisold\ny\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n\nWe can now proceed to fit the SDT models to this person’s data, beginning with the EVSDT model.\nEVSDT: one subject’s rating responses\nRecall that for the EVSDT model of binary responses, we modeled the probability p (of responding “old!” on trial i) as\n\\[p_i = \\Phi(d'\\mbox{isold}_i - c)\\]\nThis model gives the (z-scored) probability of responding “old” for new items (c = zFAR), and the increase (in z-scores) in “old” responses for old items (d’). For rating data, the model is similar but we now include multiple cs. These index the different criteria for responding with the different confidence ratings. The criteria are assumed to be ordered–people should be more lenient to say unsure old, vs. sure old, when the signal (memory strength) on that trial was weaker.\nThe EVSDT model for rating responses models the cumulative probability of responding with confidence rating k or less (\\(p(y_i \\leq k_i)\\); Decarlo (2003)):\n\\[p(y_i \\leq k_i) = \\Phi(d'\\mbox{isold}_i - c_{ki})\\]\nThis model is also known as an ordinal probit (\\(\\Phi\\)) model, and can be fit with widely available regression modeling software. (Decarlo 2003) showed how to use the PLUM procedure in SPSS to fit it for a single participant. However, we can obtain Bayesian inference for this model by estimating the model with the brms package in R (Bürkner 2017b; Stan Development Team 2016b). Ignoring prior distributions for now, the brms syntax for estimating this model with the above data is:\n\n\nfit1 <- brm(y ~ isold,\n  family = cumulative(link = \"probit\"),\n  data = d,\n  cores = 4,\n  file = \"sdtmodel3-1\"\n)\n\n\n\nThis model estimates an intercept (criterion) for each response category, and the effect of isold, which is d’. The model’s posterior distribution is summarised below:\n\n\nsummary(fit1)\n\n\n Family: cumulative \n  Links: mu = probit; disc = identity \nFormula: y ~ isold \n   Data: d (Number of observations: 1188) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]    -0.44      0.05    -0.54    -0.35 1.00     4756     3101\nIntercept[2]     0.23      0.05     0.14     0.32 1.00     5318     3355\nIntercept[3]     0.67      0.05     0.57     0.77 1.00     4556     3486\nIntercept[4]     1.20      0.06     1.10     1.31 1.00     3907     3191\nIntercept[5]     1.88      0.07     1.75     2.01 1.00     3814     3312\nisold            1.26      0.07     1.13     1.38 1.00     4059     3045\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00 1.00     4000     4000\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe five intercepts are the five criteria in the model, and isold is d’. I also estimated this model using SPSS, so it might be helpful to compare the results from these two approaches:\nPLUM y WITH x\n/CRITERIA=CIN(95) DELTA(0) LCONVERGE(0) MXITER(100) MXSTEP(5) PCONVERGE(1.0E-6) SINGULAR(1.0E-8)\n/LINK=PROBIT\n/PRINT=FIT KERNEL PARAMETER SUMMARY.\n\nParameter Estimates\n|-----------------|--------|----------|-----------------------------------|\n|                 |Estimate|Std. Error|95% Confidence Interval            |\n|                 |        |          |-----------------------|-----------|\n|                 |        |          |Lower Bound            |Upper Bound|\n|---------|-------|--------|----------|-----------------------|-----------|\n|Threshold|[y = 1]|-.442   |.051      |-.541                  |-.343      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 2]|.230    |.049      |.134                   |.326       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 3]|.669    |.051      |.569                   |.769       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 4]|1.198   |.056      |1.088                  |1.308      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 5]|1.876   |.066      |1.747                  |2.005      |\n|---------|-------|--------|----------|-----------------------|-----------|\n|Location |x      |1.253   |.065      |1.125                  |1.381      |\n|-------------------------------------------------------------------------|\nLink function: Probit.\nUnsurprisingly, the numerical results from brms (posterior means and standard deviations, credibility intervals) match the frequentist ones obtained from SPSS under these conditions.\nWe can now illustrate graphically how the estimated parameters map to the signal detection model. d’ is the separation of the signal and noise distributions’ peaks: It indexes the subject’s ability to discriminate signal from noise trials. The five intercepts are the (z-scored) criteria for responding with the different confidence ratings. If we convert the z-scores to proportions (using R’s pnorm() for example), they measure the (cumulative) area under the noise distribution to the left of that z-score. The model is visualized in Figure 6.\n\n\n\nFigure 6: The equal variance Gaussian signal detection model, visualized from the parameters’ posterior means. The two distributions are the noise distribution (dashed) and the signal distribution (solid). Dotted vertical lines are response criteria. d’ is the distance between the peaks of the two distributions.\n\n\n\nUVSDT: one subject’s rating responses\nNotice that the above model assumed that the noise and signal distributions have the same variance. The unequal variances SDT (UVSDT) model allows the signal distribution to have a different variance than the noise distribution (whose standard deviation is still arbitrarily fixed at 1). It has been found that when the signal distribution’s standard deviation is allowed to vary, it is consistently greater than 1.\nThe UVSDT model adds one parameter, and we can write out the resulting model by including the signal distribution’s standard deviation as a scale parameter in the above equation (Decarlo 2003). However, because the standard deviation parameter must be greater than zero, it is convenient to model \\(\\mbox{log}(\\sigma_{old}) = a\\) instead:\n\\[p(y_i \\leq k_i) = \\Phi(\\frac{d'\\mbox{isold}_i - c_k}{\\mbox{exp}(a\\mbox{isold}_i)})\\]\nIt turns out that this nonlinear model—also knows as a probit model with heteroscedastic error (e.g. DeCarlo (2010))—can be estimated with brms. Initially, I thought that we could write out a nonlinear brms formula for the ordinal probit model, but brms does not support nonlinear cumulative ordinal models. I then proceeded to modify the raw Stan code to estimate this model, but although that worked, it would be less practical for applied work because not everyone wants to go through the trouble of writing Stan code.\nAfter some back and forth with the creator of brms—Paul Bürkner, who deserves a gold medal for his continuing hard work on this free and open-source software—I found out that brms by default includes a similar parameter in ordinal regression models. If you scroll back up and look at the summary of fit1, at the top you will see that the model’s formula is:\nFormula: y ~ isold \ndisc = 1\nIn other words, there is a “discrimination” parameter disc, which is set to 1 by default. Here’s how brms parameterizes the ordinal probit model:\n\\[p(y_i \\leq k_i) = \\Phi(disc * (c_{ki} - d'\\mbox{isold}_i))\\]\nImportantly, we can also include predictors on disc. In this case, we want to estimate disc when isold is 1, such that disc is 1 for new items, but estimated from data for old items. This parameter is by default modelled through a log link function, and including a 0/1 predictor (isold) will therefore work fine:\n\\[p(y_i \\leq k_i) = \\Phi(\\mbox{exp}(disc\\mbox{isold}_i) * (c_{ki} - d'\\mbox{isold}_i))\\]\nWe can therefore estimate this model with only a small tweak to the EVSDT model’s code:\n\n\nuvsdt_m <- bf(y ~ isold, disc ~ 0 + isold)\n\n\n\nThere are two brms formulas in the model. The first, y ~ isold is already familiar to us. In the second formula, we write disc ~ 0 + isold to prevent the parameter from being estimated for the noise distribution: Recall that we have set the standard deviation of the noise distribution to be one (achieved by \\(exp(disc * \\mbox{0}) = 1\\)). In R’s (and by extension, brms’) modeling syntax 0 + ... means removing the intercept from the model. By including isold only, we achieve the 0/1 predictor as described above. We can then estimate the model:\n\n\nfit2 <- brm(uvsdt_m,\n  family = cumulative(link = \"probit\"),\n  data = d,\n  control = list(adapt_delta = .99),\n  cores = 4,\n  file = \"sdtmodel3-2\"\n)\n\n\n\nThe model’s estimated parameters:\n\n\nsummary(fit2)\n\n\n Family: cumulative \n  Links: mu = probit; disc = log \nFormula: y ~ isold \n         disc ~ 0 + isold\n   Data: d (Number of observations: 1188) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]    -0.54      0.05    -0.65    -0.43 1.00     3368     2690\nIntercept[2]     0.20      0.05     0.10     0.30 1.00     5639     3303\nIntercept[3]     0.71      0.05     0.61     0.81 1.00     4686     3152\nIntercept[4]     1.37      0.07     1.24     1.51 1.00     2213     2560\nIntercept[5]     2.31      0.11     2.09     2.54 1.00     1395     1995\nisold            1.53      0.10     1.35     1.73 1.00     1746     2314\ndisc_isold      -0.36      0.06    -0.48    -0.23 1.00     1466     2548\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nNotice that we need to flip the sign of the disc parameter to get \\(\\mbox{log}(\\sigma_{old})\\). Exponentiation gives us the standard deviation of the signal distribution, and because we estimated the model in the Bayesian framework, our estimate of this parameter is a posterior distribution, plotted on the y-axis of Figure 7.\n\n\n\nFigure 7: The (approximate) joint posterior density of two UVSDT parameters (d’ and standard deviation of the signal distribution) fitted to one participant’s data. Lighter yellow colors indicate higher posterior density. Red point shows the maximum likelihood estimates obtained from SPSS’s ordinal regression module.\n\n\n\nWe can also compare the results from brms’ to ones obtained from SPSS (SPSS procedure described in (Decarlo 2003)):\nPLUM y WITH x\n/CRITERIA=CIN(95) DELTA(0) LCONVERGE(0) MXITER(100) MXSTEP(5) PCONVERGE(1.0E-6) SINGULAR(1.0E-8)\n/LINK=PROBIT\n/PRINT=FIT KERNEL PARAMETER SUMMARY\n/SCALE=x .\n\nParameter Estimates\n|-----------------|--------|----------|-----------------------------------|\n|                 |Estimate|Std. Error|95% Confidence Interval            |\n|                 |        |          |-----------------------|-----------|\n|                 |        |          |Lower Bound            |Upper Bound|\n|---------|-------|--------|----------|-----------------------|-----------|\n|Threshold|[y = 1]|-.533   |.054      |-.638                  |-.428      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 2]|.204    |.050      |.107                   |.301       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 3]|.710    |.053      |.607                   |.813       |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 4]|1.366   |.067      |1.235                  |1.498      |\n|         |-------|--------|----------|-----------------------|-----------|\n|         |[y = 5]|2.294   |.113      |2.072                  |2.516      |\n|---------|-------|--------|----------|-----------------------|-----------|\n|Location |x      |1.519   |.096      |1.331                  |1.707      |\n|---------|-------|--------|----------|-----------------------|-----------|\n|Scale    |x      |.348    |.063      |.225                   |.472       |\n|-------------------------------------------------------------------------|\nLink function: Probit.\nAgain, the maximum likelihood estimates (SPSS) match our Bayesian quantities numerically, because we used uninformative prior distributions. Plotting the model’s implied distributions illustrates that the signal distribution has greater variance than the noise distribution (Figure 8).\n\n\n\nFigure 8: The unequal variance Gaussian signal detection model, visualized from the parameters’ posterior means. The two distributions are the noise distribution (dashed) and the signal distribution (solid). Dotted vertical lines are response criteria. d’ is the scaled distance between the peaks of the two distributions.\n\n\n\nAdditional quantities of interest can be calculated from the parameters’ posterior distributions. One benefit of obtaining samples from the posterior is that if we complete these calculations row-wise, we automatically obtain (samples from) the posterior distributions of these additional quantities.\nHere, we calculate one such quantity: The ratio of the noise to signal standard deviations (\\(\\mbox{exp}(-a)\\); notice that our model returns -a as disc_isold), which is also the slope of the z-ROC curve. We’ll first obtain the posterior samples of disc_isold, then calculate the ratio, and summarize the samples from ratio’s posterior distribution with their 2.5%, 50%, and 97.5%iles:\n\n\nas.data.frame(fit2, pars = \"b_disc_isold\") %>%\n  transmute(ratio = exp(b_disc_isold)) %>%\n  pull(ratio) %>%\n  quantile(probs = c(.025, .5, .975))\n\n\n     2.5%       50%     97.5% \n0.6160153 0.7004072 0.7926746 \n\nThese summaries are the parameter’s 95% Credible interval and median, and as such can be used to summarize this quantity in a publication. We could also visualize the posterior draws as a histogram:\n\n\nas.data.frame(fit2, pars = \"b_disc_isold\") %>%\n  transmute(ratio = exp(b_disc_isold)) %>%\n  ggplot(aes(ratio)) +\n  geom_histogram(col = \"black\", fill = \"gray70\") +\n  scale_y_continuous(expand = expansion(c(0, .1))) +\n  theme(aspect.ratio = 1)\n\n\n\n\nUVSDT for multiple participants\nAbove, we fit the UVSDT model for a single subject. However, we almost always want to discuss our inference about the population, not individual subjects. Further, if we wish to discuss individual subjects, we should place them in the context of other subjects. A multilevel (aka hierarchical, mixed) model accomplishes these goals by including population- and subject-level parameters.\nExample data set\nWe’ll use a data set of 48 subjects’ confidence ratings on a 6 point scale: 1 = “sure new,” …, 6 = “sure old” (Koen et al. 2013). This data set is included in the R package MPTinR (Singmann and Kellen 2013).\nIn this experiment (Koen et al. 2013), participants completed a study phase, and were then tested under full attention, or while doing a second task. Here, we focus on the rating data provided in the full attention condition. Below, I reproduce the aggregate rating counts for old and new items from the Table in the article’s appendix. (It is useful to ensure that we are indeed using the same data.)\n\nTable 7: Example data from Koen et al. (2013)\nisold\n6\n5\n4\n3\n2\n1\nold\n2604\n634\n384\n389\n422\n309\nnew\n379\n356\n454\n871\n1335\n1365\n\nFor complete R code, including pre-processing the data, please refer to the source code of this blog post. I have omitted some of the less important code from the blog post for clarity.\nModel syntax\nHere’s the brms syntax we used for estimating the model for a single participant:\n\n\nuvsdt_m <- bf(y ~ isold, disc ~ 0 + isold)\n\n\n\nWith the above syntax we specifed seven parameters: Five intercepts (aka ‘thresholds’ in the cumulative probit model) on y3; the effect of isold on y; and the effect of isold on the discrimination parameter disc4. There are five intercepts (thresholds), because there are six response categories.\nWe extend the code to a hierarchical model by specifying that all these parameters vary across participants (variable id in the data).\n\n\nuvsdt_h <- bf(\n  y ~ isold + (isold | s | id),\n  disc ~ 0 + isold + (0 + isold | s | id)\n)\n\n\n\nRecall from above that using |s| leads to estimating correlations among the varying effects. There will only be one standard deviation associated with the thresholds; that is, the model assumes that subjects vary around the mean threshold similarly for all thresholds.\nPrior distributions\nI set a N(1, 3) prior on dprime, just because I know that in these tasks performance is usually pretty good. Perhaps this prior is also influenced by my reading of the paper! I also set a N(0, 1) prior on a: Usually this parameter is found to be around \\(-\\frac{1}{4}\\), but I’m ignoring that information.\nThe t(7, 0, .33) priors on the between-subject standard deviations reflect my assumption that the subjects should be moderately similar to one another, but also allows larger deviations. (They are t-distributions with seven degrees of freedom, zero mean, and .33 standard deviation.)\n\n\nPrior <- c(\n  prior(normal(1, 3), class = \"b\", coef = \"isold\"),\n  prior(normal(0, 1), class = \"b\", coef = \"isold\", dpar = \"disc\"),\n  prior(student_t(7, 0, .33), class = \"sd\"),\n  prior(student_t(7, 0, .33), class = \"sd\", dpar = \"disc\"),\n  prior(lkj(2), class = \"cor\")\n)\n\n\n\nEstimate and summarise parameters\nWe can then estimate the model as before. Be aware that this model takes quite a bit longer to estimate, so for this example I have set only 500 HMC iterations.\n\n\nfit <- brm(uvsdt_h,\n  family = cumulative(link = \"probit\"),\n  data = d,\n  prior = Prior,\n  control = list(adapt_delta = .9), inits = 0,\n  cores = 4, iter = 500,\n  file = \"sdtmodel4-1\"\n)\n\n\n\nWe then display numerical summaries of the model’s parameters. Note that the effective sample sizes are modest, and Rhats indicate that we would benefit from drawing more samples from the posterior. For real applications, I recommend more than 500 iterations per chain.\n\n\nsummary(fit)\n\n\n Family: cumulative \n  Links: mu = probit; disc = log \nFormula: y ~ isold + (isold | s | id) \n         disc ~ 0 + isold + (0 + isold | s | id)\n   Data: d (Number of observations: 9502) \nSamples: 4 chains, each with iter = 500; warmup = 250; thin = 1;\n         total post-warmup samples = 1000\n\nGroup-Level Effects: \n~id (Number of levels: 48) \n                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)                 0.34      0.04     0.28     0.43 1.01      252      364\nsd(isold)                     0.78      0.10     0.60     1.01 1.01      153      202\nsd(disc_isold)                0.46      0.05     0.37     0.56 1.03      183      269\ncor(Intercept,isold)         -0.45      0.13    -0.65    -0.16 1.02      167      319\ncor(Intercept,disc_isold)     0.34      0.13     0.08     0.57 1.02      198      454\ncor(isold,disc_isold)        -0.76      0.07    -0.88    -0.59 1.02      217      381\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]    -0.59      0.05    -0.69    -0.49 1.03      169      364\nIntercept[2]     0.21      0.05     0.11     0.30 1.03      167      405\nIntercept[3]     0.71      0.05     0.61     0.80 1.03      158      356\nIntercept[4]     1.05      0.05     0.95     1.14 1.03      160      323\nIntercept[5]     1.50      0.05     1.39     1.60 1.03      164      411\nisold            1.86      0.12     1.63     2.10 1.08       51      194\ndisc_isold      -0.38      0.07    -0.51    -0.25 1.04      101      304\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nLet’s first focus on the “Population-level Effects”: The effects for the “average person.” isold is d’, and is very close to the one reported in the paper (eyeballing Figure 3 in Koen et al. (2013); this d’ is not numerically reported in the paper). disc_isold is, because of the model’s parameterization, \\(-\\mbox{log}(\\sigma_{signal}) = -a\\). The paper discusses \\(V_o = \\sigma_{signal}\\), and therefore we transform each posterior sample of our -a to obtain samples from \\(V_o\\)’s posterior distribution.\n\n\nsamples <- posterior_samples(fit, \"b_\") %>%\n  mutate(Vo = exp(-b_disc_isold))\n\n\n\nWe can then plot density curves (Gabry 2017) for each of the Population-level Effects in our model, including \\(V_o\\). Figure 9 shows that our estimate of \\(V_o\\) corresponds very closely to the one reported in the paper (Figure 3 in Koen et al. (2013)).\n\n\nmcmc_areas(samples, point_est = \"mean\", prob = .8)\n\n\n\n\nFigure 9: Density plots of UVSDT model’s Population-level Effects’ posterior distributions. Different parameters are indicated on the y-axis, and possible values on the x-axis. Vertical lines are posterior means, and shaded areas are 80% credible intervals.\n\n\n\nHeterogeneity parameters\nAlthough the “population-level estimates,” which perhaps should be called “average effects,” are usually the main target of inference, they are not the whole story, nor are they necessarily the most interesting part of it. It has been firmly established that, when allowed to vary, the standard deviation of the signal distribution is greater than 1. However, the between-subject variability of this parameter has received less interest. Figure 10 reveals that the between-subject heterogeneity of a is quite large: The subject-specific effects have a standard deviation around .5.\n\n\nsamples_h <- posterior_samples(fit, c(\"sd_\", \"cor_\"))\nmcmc_areas(samples_h, point_est = \"mean\", prob = .8)\n\n\n\n\nFigure 10: Density plots of the standard deviation and correlation parameters of the UVSDT model’s parameters. Parameter’s appended with ’sd_id__’ are between-id standard deviations, ones with ’cor_id__’ are between-id correlations.\n\n\n\nFigure 10 also tells us that the subject-specific d’s and as are correlated (\"cor_id__isold__disc_isold\"). We can further investigate this relationship by plotting the subject specific signal-SDs and d’s side by side:\n\n\n\nFigure 11: Ridgeline plot of posterior distributions of subject-specific standard deviations (left) and d-primes (right). The ordering of subjects on the y-axis is the same, so as to highlight the relationship between the two variables.\n\n\n\nAs can be seen in the ridgeline plots (Wilke 2017) in Figure 11, participants with greater \\(\\sigma_{signal}\\) tend to have greater d’: Increase in recognition sensitivity is accompanied with an increase in the signal distribution’s variability. The density plots also make it clear that we are much less certain about individuals whose values (either one) are greater, as shown by the spread out posterior distributions. Yet another way to visualize this relationship is with a scatterplot of the posterior means Figure 12.\n\n\n\nFigure 12: Scatterplot of posterior means of subject-specific d-primes and signal distribution standard deviations.\n\n\n\nConclusion\nEstimating EVSDT and UVSDT models in the Bayesian framework with the brms package (Bürkner 2017b) is both easy (relatively speaking) and informative. In this post, we estimated a hierarchical nonlinear cognitive model using no more than a few lines of code. Previous literature on the topic (e.g. Rouder et al. (2007)) has focused on simpler (EVSDT) models with more complicated implementations–hopefully in this post I have shown that these models are within the reach of a greater audience, provided that they have some familiarity with R.\nAnother point worth making is a more general one about hierarchical models: We know that participants introduce (random) variation in our models. Ignoring this variation is clearly not good (Estes 1956). It is more appropriate to model this variability, and use the resulting parameters to draw inference about the heterogeneity in parameters (and more generally, cognitive strategies) across individuals. Although maximum likelihood methods provide (noisy) point estimates of what I’ve here called between-subject heterogeneity parameters, the Bayesian method allows drawing firm conclusions about these parameters.\n\nSupport this work\n\nSoftware used\nThe following software packages were used: R [Version 4.0.3; R Core Team (2020)] and the R-packages bayesplot [Version 1.8.0; Gabry et al. (2019)], boot [Version 1.3.27; Davison and Hinkley (1997)], brms [Version 2.14.4; Bürkner (2017a); Bürkner (2018)], dplyr [Version 1.0.4; Wickham et al. (2021)], forcats [Version 0.5.1; Wickham (2021a)], ggplot2 [Version 3.3.3; Wickham (2016)], ggridges [Version 0.5.3; Wilke (2021)], knitr [Version 1.31; Xie (2015)], lme4 [Version 1.1.26; Bates et al. (2015)], Matrix [Version 1.3.2; Bates and Maechler (2021)], purrr [Version 0.3.4; Henry and Wickham (2020)], Rcpp [Version 1.0.6; Eddelbuettel and François (2011); Eddelbuettel and Balamuta (2018)], readr [Version 1.4.0; Wickham and Hester (2020)], scales [Version 1.1.1; Wickham and Seidel (2020)], sdtalt [Version 1.3; Wright (2011b)], stringr [Version 1.4.0; Wickham (2019)], tibble [Version 3.1.0; Müller and Wickham (2021)], tidyr [Version 1.1.3; Wickham (2021b)], tidyverse [Version 1.3.0; Wickham et al. (2019)], viridis [Version 0.5.1; Garnier (2018a); Garnier (2018b)], and viridisLite [Version 0.3.0; Garnier (2018b)].\n\n\n\n\nBates, Douglas, and Martin Maechler. 2021. Matrix: Sparse and Dense Matrix Classes and Methods. https://CRAN.R-project.org/package=Matrix.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBolger, Niall, and Jean-Philippe Laurenceau. 2013. Intensive Longitudinal Methods: An Introduction to Diary and Experience Sampling Research. Guilford Press. http://www.intensivelongitudinal.com/.\n\n\nBürkner, Paul-Christian. 2017a. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\n———. 2017b. “Brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\n———. 2018. “Advanced Bayesian Multilevel Modeling with the R Package brms.” The R Journal 10 (1): 395–411. https://doi.org/10.32614/RJ-2018-017.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their Applications. Cambridge: Cambridge University Press. http://statwww.epfl.ch/davison/BMA/.\n\n\nDecarlo, Lawrence T. 2003. “Using the PLUM Procedure of SPSS to Fit Unequal Variance and Generalized Signal Detection Models.” Behavior Research Methods, Instruments, & Computers 35 (1): 49–56. https://doi.org/10.3758/BF03195496.\n\n\nDeCarlo, Lawrence T. 1998. “Signal Detection Theory and Generalized Linear Models.” Psychological Methods 3 (2): 186–205. https://doi.org/10.1037/1082-989X.3.2.186.\n\n\n———. 2010. “On the Statistical and Theoretical Basis of Signal Detection Theory and Extensions: Unequal Variance, Random Coefficient, and Mixture Models.” Journal of Mathematical Psychology 54 (3): 304–13. https://doi.org/10.1016/j.jmp.2010.01.001.\n\n\nEddelbuettel, Dirk, and James Joseph Balamuta. 2018. “Extending extitR with extitC++: A Brief Introduction to extitRcpp.” The American Statistician 72 (1): 28–36. https://doi.org/10.1080/00031305.2017.1375990.\n\n\nEddelbuettel, Dirk, and Romain François. 2011. “Rcpp: Seamless R and C++ Integration.” Journal of Statistical Software 40 (8): 1–18. https://doi.org/10.18637/jss.v040.i08.\n\n\nEstes, W. K. 1956. “The Problem of Inference from Curves Based on Group Data.” Psychological Bulletin 53 (2): 134–40. https://doi.org/10.1037/h0045156.\n\n\nGabry, Jonah. 2017. Bayesplot: Plotting for Bayesian Models. http://mc-stan.org/.\n\n\nGabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. “Visualization in Bayesian Workflow.” J. R. Stat. Soc. A 182: 389–402. https://doi.org/10.1111/rssa.12378.\n\n\nGarnier, Simon. 2018a. Viridis: Default Color Maps from ’Matplotlib’. https://CRAN.R-project.org/package=viridis.\n\n\n———. 2018b. viridisLite: Default Color Maps from ’Matplotlib’ (Lite Version). https://CRAN.R-project.org/package=viridisLite.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nKoen, Joshua D., Mariam Aly, Wei-Chun Wang, and Andrew P. Yonelinas. 2013. “Examining the Causes of Memory Strength Variability: Recollection, Attention Failure, or Encoding Variability?” Journal of Experimental Psychology: Learning, Memory, and Cognition 39 (6): 1726–41. https://doi.org/10.1037/a0033671.\n\n\nKruschke, John K. 2014. Doing Bayesian Data Analysis: A Tutorial Introduction with r. 2nd Edition. Burlington, MA: Academic Press.\n\n\nMacmillan, Neil A., and C. Douglas Creelman. 2005. Detection Theory: A User’s Guide. 2nd ed. Mahwah, N.J: Lawrence Erlbaum Associates.\n\n\nMcElreath, Richard. 2016. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC Press.\n\n\nMüller, Kirill, and Hadley Wickham. 2021. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nR Core Team. 2017. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\n———. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRouder, Jeffrey N., and Jun Lu. 2005. “An Introduction to Bayesian Hierarchical Models with an Application in the Theory of Signal Detection.” Psychonomic Bulletin & Review 12 (4): 573–604. https://doi.org/10.3758/BF03196750.\n\n\nRouder, Jeffrey N., Jun Lu, Dongchu Sun, Paul Speckman, Richard D. Morey, and Moshe Naveh-Benjamin. 2007. “Signal Detection Models with Random Participant and Item Effects.” Psychometrika 72 (4): 621–42. https://doi.org/10.1007/s11336-005-1350-6.\n\n\nSingmann, Henrik, and David Kellen. 2013. “MPTinR: Analysis of Multinomial Processing Tree Models in R.” Behavior Research Methods 45 (2): 560–75. https://doi.org/10.3758/s13428-012-0259-0.\n\n\nSkagerberg, Elin M., and Daniel B. Wright. 2008. “Manipulating Power Can Affect Memory Conformity.” Applied Cognitive Psychology 22 (2): 207–16. https://doi.org/10.1002/acp.1353.\n\n\nStan Development Team. 2016a. RStan: The r Interface to Stan. http://mc-stan.org/.\n\n\n———. 2016b. Stan: A c++ Library for Probability and Sampling, Version 2.15.0. http://mc-stan.org/.\n\n\nStanislaw, Harold, and Natasha Todorov. 1999. “Calculation of Signal Detection Theory Measures.” Behavior Research Methods, Instruments, & Computers 31 (1): 137–49. http://link.springer.com/article/10.3758/BF03207704.\n\n\nvan Ravenzwaaij, Don, Pete Cassey, and Scott D. Brown. 2016. “A Simple Introduction to Markov Chain Monte–Carlo Sampling.” Psychonomic Bulletin & Review, March, 1–12. https://doi.org/10.3758/s13423-016-1015-8.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2021a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2021b. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, and Dana Seidel. 2020. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nWilke, Claus O. 2017. Ggridges: Ridgeline Plots in ’Ggplot2’. https://CRAN.R-project.org/package=ggridges.\n\n\n———. 2021. Ggridges: Ridgeline Plots in ’Ggplot2’. https://CRAN.R-project.org/package=ggridges.\n\n\nWright, Daniel B. 2011a. Sdtalt: Signal Detection Theory and Alternatives. https://CRAN.R-project.org/package=sdtalt.\n\n\n———. 2011b. Sdtalt: Signal Detection Theory and Alternatives. https://CRAN.R-project.org/package=sdtalt.\n\n\nXie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\nHierarchical regression is sometimes used to mean the practice of adding predictors to a regression model based on the predictors’ p-values. Whatever you do, don’t do that.↩︎\nThe label “Group-Level Effects” might be slightly confusing because the SD and correlation parameters describe the population of subject-specific effects. I have yet to find a 100% satisfactory terminology here, but think that brms’ terminology is certainly less confusing than that of “random” and “fixed” effects, traditionally encountered in multilevel modeling literature.↩︎\nRecall that intercepts are automatically included, but can be explicitly included by adding 1 to the formula’s right hand side.↩︎\n0 + ... removes the model’s intercept.↩︎\n",
    "preview": "posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/2017-10-09-bayesian-estimation-of-signal-detection-theory-models_files/figure-html5/densityplot-1.png",
    "last_modified": "2021-03-09T21:01:49+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2017-03-21-bayes-factors-with-brms/",
    "title": "Bayes Factors with brms",
    "description": "How to calculate Bayes Factors with the R package brms using the Savage-Dickey density ratio method.",
    "author": [
      {
        "name": "Matti Vuorre",
        "url": "https://vuorre.netlify.com"
      }
    ],
    "date": "2017-03-21",
    "categories": [
      "statistics",
      "tutorial",
      "R",
      "brms"
    ],
    "contents": "\n\nContents\nExample 0\nExample 1: Equality of Proportions\nExample 2: Hierarchical Bayesian one-sample proportion test\nConclusion\n\nHere’s a short post on how to calculate Bayes Factors with the R package brms using the Savage-Dickey density ratio method (Wagenmakers et al. 2010).\nTo get up to speed with what the Savage-Dickey density ratio method is–or what Bayes Factors are–please read the target article (Wagenmakers et al. 2010). (The paper is available on the author’s webpage.) Here, I’ll only show the R & brms code to do the calculations discussed in Wagenmakers et al. (2010). In their paper, they used WinBUGS, which requires quite a bit of code to sample from even a relatively simple model. brms on the other hand uses the familiar R formula syntax, making it easy to use. brms also does the MCMC sampling with Stan (Stan Development Team 2016), or rather creates Stan code from a specified R model formula by what can only be described as string processing magic, making the sampling very fast. Let’s get straight to the examples. We will use these packages:\n\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(brms)\nlibrary(patchwork)\nlibrary(tidyverse)\n\n\n\nExample 0\nWagenmakers and colleagues begin with a simple example of 10 true/false questions: We observe a person answering 9 (s) out of 10 (k) questions correctly.\n\n\nd <- data.frame(s = 9, k = 10)\n\n\n\nWe are interested in the person’s latent ability to answer similar questions correctly. This ability is represented by \\(\\theta\\) (theta), which for us will be the probability parameter (sometimes also called the rate parameter) in a binomial distribution. The maximum likelihood (point) estimate for \\(\\theta\\) is the proportion n/k = .9.\nThe first thing we’ll need to specify with respect to our statistical model is the prior probability distribution for \\(\\theta\\). As in Wagenmakers et al. 2010, we specify a uniform prior, representing no prior information about the person’s ability to aswer the questions. For the binomial probability parameter, \\(Beta(\\alpha = 1, \\beta = 1)\\) is a uniform prior.\n\n\npd <- tibble(\n  x = seq(0, 1, by = .01),\n  Prior = dbeta(x, 1, 1)\n)\n\n\n\n\nThe solid line represents the probability density assigned to values of \\(\\theta\\) by this prior probability distribution. You can see that it is 1 for all possible parameter values: They are all equally likely a priori. For this simple illustration, we can easily calculate the posterior distribution by adding the number of correct and incorrect answers to the parameters of the prior Beta distribution.\n\n\npd$Posterior <- dbeta(pd$x, 9+1, 1+1)\n\n\n\n\nThe Savage-Dickey density ratio is calculated by dividing the posterior density by the prior density at a specific parameter value. Here, we are interested in .5, a “null hypothesis” value indicating that the person’s latent ability is .5, i.e. that they are simply guessing.\n\n\nTable 1: Bayes Factors for first example.\n\n\nx\n\n\nPrior\n\n\nPosterior\n\n\nBF01\n\n\nBF10\n\n\n0.5\n\n\n1\n\n\n0.107\n\n\n0.107\n\n\n9.309\n\n\nOK, so in this example we are able to get to the posterior with simply adding values into the parameters of the Beta distribution, but let’s now see how to get to this problem using brms. First, here’s the brms formula of the model:\n\n\nm0 <- bf(\n  s | trials(k) ~ 0 + Intercept,\n  family = binomial(link = \"identity\")\n)\n\n\n\nRead the first line as “s successes from k trials regressed on intercept.” That’s a little clunky, but bear with it. If you are familiar with R’s modeling syntax, you’ll be wondering why we didn’t simply specify ~ 1 (R’s default notation for an intercept). The reason is that brms by default uses a little trick in parameterizing the intercept which speeds up the MCMC sampling. In order to specify a prior for the intercept, you’ll have to take the default intercept out (0 +), and use the reserved string intercept to say that you mean the regular intercept. See ?brmsformula for details. (For this model, with only one parameter, this complication doesn’t matter, but I wanted to introduce it early on so that you’d be aware of it when estimating multi-parameter models.)\nThe next line specifies that the data model is binomial, and that we want to model it’s parameter through an identity link. Usually when you model proportions or binary data, you’d use a logistic (logistic regression!), probit or other similar link function. In fact this is what we’ll do for later examples. Finally, we’ll use the data frame d.\nOK, then we’ll want to specify our priors. Priors are extremo important for Bayes Factors–and probabilistic inference in general. To help set priors, we’ll first call get_priors() with the model information, which is basically like asking brms to tell what are the possible priors, and how to specify then, given this model.\n\n\nget_prior(m0, data = d)\n\n\n  prior class      coef group resp dpar nlpar bound       source\n (flat)     b                                            default\n (flat)     b Intercept                             (vectorized)\n\nThe first line says that there is only one class of parameters b, think of class b as “betas” or “regression coefficients.” The second line says that the b class has only one parameter, the intercept. So we can set a prior for the intercept, and this prior can be any probability distribution in Stan language. We’ll create this prior using brms’ set_prior(), give it a text string representing the Beta(1, 1) prior for all parameters of class b (shortcut, could also specify that we want it for the intercept specifically), and then say the upper and lower bounds (\\(\\theta\\) must be between 0 and 1).\n\n\nPrior <- set_prior(\"beta(1, 1)\", class = \"b\", lb = 0, ub = 1)\n\n\n\nAlmost there. Now we’ll actually sample from the model using brm(), give it the model, priors, data, ask it to sample from priors (for the density ratio), and set a few extra MCMC parameters.\n\n\nm <- brm(\n  formula = m0,\n  prior = Prior,\n  data = d,\n  sample_prior = TRUE,\n  iter = 1e4,\n  cores = 4,\n  file = \"bayesfactormodel\"\n)\n\n\n\nWe can get the estimated parameter by asking the model summary:\n\n\nsummary(m)\n\n\n Family: binomial \n  Links: mu = identity \nFormula: s | trials(k) ~ 0 + Intercept \n   Data: d (Number of observations: 1) \nSamples: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup samples = 20000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.83      0.10     0.59     0.98 1.00     6161     6317\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe Credible Interval matches exactly what’s reported in the paper. The point estimate differs slightly because here we see the posterior mean, whereas in the paper, Wagenmakers et al. report the posterior mode. I’ll draw a line at their posterior mode, below, to show that it matches.\n\n\nsamples <- posterior_samples(m, \"b\")\n\n\n\nTable 2: Six first rows of posterior samples.\n\n\nb_Intercept\n\n\nprior_b\n\n\n0.90\n\n\n0.97\n\n\n0.78\n\n\n0.11\n\n\n0.76\n\n\n0.67\n\n\n0.76\n\n\n0.43\n\n\n0.87\n\n\n0.89\n\n\n0.86\n\n\n0.50\n\n\n\nWe can already see the densities, so all that’s left is to obtain the exact values at the value of interest (.5) and take the \\(\\frac{posterior}{prior}\\) ratio. Instead of doing any of this by hand, we’ll use brms’ function hypothesis() that allows us to test point hypotheses using the Dickey Savage density ratio. For this function we’ll need to specify the point of interest, .5, as the point hypothesis to be tested.\n\n\nh <- hypothesis(m, \"Intercept = 0.5\")\nprint(h, digits = 4)\n\n\nHypothesis Tests for class b:\n             Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (Intercept)-(0.5) = 0   0.3338    0.1031   0.0889   0.4778     0.1176    0.1052    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nThe Evid.Ratio is our Bayes Factor BF01. Notice that it matches the value 0.107 pretty well. You can also plot this hypothesis object easily with the plot() method:\n\n\nplot(h)\n\n\n\n\nOK, so that was a lot of work for such a simple problem, but the real beauty of brms (and Stan) is the scalability: We can easily solve a problem with one row of data and one parameter, and it won’t take much more to solve a problem with tens of thousands of rows of data, and hundreds of parameters. Let’s move on to the next example from Wagenmakers et al. (2010).\nExample 1: Equality of Proportions\nThese are the data from the paper\n\n\nd <- data.frame(\n  pledge = c(\"yes\", \"no\"),\n  s = c(424, 5416),\n  n = c(777, 9072)\n)\nd\n\n\n  pledge    s    n\n1    yes  424  777\n2     no 5416 9072\n\nThey use Beta(1, 1) priors for both rate parameters, which we’ll do as well. Notice that usually a regression formula has an intercept and a coefficient (e.g. effect of group.) By taking the intercept out (0 +) we can define two pledger-group proportions instead, and set priors on these. If we used an intercept + effect formula, we could set a prior on the effect itself.\n\n\nm1 <- bf(\n  s | trials(n) ~ 0 + pledge,\n  family = binomial(link = \"identity\")\n)\nget_prior(\n  m1,\n  data = d\n)\n\n\n  prior class      coef group resp dpar nlpar bound       source\n (flat)     b                                            default\n (flat)     b  pledgeno                             (vectorized)\n (flat)     b pledgeyes                             (vectorized)\n\nWe can set the Beta prior for both groups’ rate with one line of code by setting the prior on the b class without specifying the coef.\n\n\nPrior <- set_prior(\"beta(1, 1)\", class = \"b\", lb = 0, ub = 1)\n\n\n\nLike above, let’s estimate.\n\n\nm1 <- brm(\n  m1,\n  prior = Prior,\n  sample_prior = TRUE,\n  iter = 1e4,\n  data = d,\n  cores = 4,\n  file = \"bayesfactormodel2\"\n)\n\n\n\nOur estimates match the MLEs reported in the paper:\n\n\nsummary(m1)\n\n\n Family: binomial \n  Links: mu = identity \nFormula: s | trials(n) ~ 0 + pledge \n   Data: d (Number of observations: 2) \nSamples: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup samples = 20000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\npledgeno      0.60      0.01     0.59     0.61 1.00    17032    13364\npledgeyes     0.55      0.02     0.51     0.58 1.00    17551    12410\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nTo get the density ratio Bayes Factor, we’ll need to specify a text string as our hypothesis. Our hypothesis is that the rate parameters \\(\\theta_1\\) and \\(\\theta_2\\) are not different: \\(\\theta_1\\) = \\(\\theta_2\\). The alternative, then, is the notion that the parameter values differ.\n\n\nh1 <- hypothesis(m1, \"pledgeyes = pledgeno\")\nh1\n\n\nHypothesis Tests for class b:\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (pledgeyes)-(pled... = 0    -0.05      0.02    -0.09    -0.02       0.51      0.34    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nAs noted in the paper, a difference value of 0 is about twice as well supported before seeing the data, i.e. the null hypothesis of no difference is twice less likely after seeing the data:\n\n\n1 / h1$hypothesis$Evid.Ratio # BF10\n\n\n[1] 1.950406\n\nThe paper reports BF01 = 0.47, so we’re getting the same results (as we should.) You can also compare this figure to what’s reported in the paper.\n\n\nh1p1 <- plot(h1, plot = F)[[1]]\nh1p2 <- plot(h1, plot = F)[[1]] +\n  coord_cartesian(xlim = c(-.05, .05), ylim = c(0, 5))\n  \n(h1p1 | h1p2) +\n  plot_layout(guides = \"collect\")\n\n\n\n\nMoving right on to Example 2, skipping the section on “order restricted analysis.”\nExample 2: Hierarchical Bayesian one-sample proportion test\nThe data for example 2 is not available, but we’ll simulate similar data. The simulation assumes that the neither-primed condition average correct probability is 50%, and that the both-primed condition benefit is 5%. Obviously, the numbers here won’t match anymore, but the data reported in the paper has an average difference in proportions of about 4%.\n\n\nset.seed(5)\nd <- tibble(\n  id = c(rep(1:74, each = 2)),\n  primed = rep(c(\"neither\", \"both\"), times = 74),\n  prime = rep(c(0, 1), times = 74), # Dummy coded\n  n = 21,\n  correct = rbinom(74 * 2, 21, .5 + prime * .05)\n)\ngroup_by(d, primed) %>% summarize(p = sum(correct) / sum(n))\n\n\n# A tibble: 2 x 2\n  primed      p\n* <chr>   <dbl>\n1 both    0.542\n2 neither 0.499\n\nThis data yields a similar t-value as in the paper.\n\n\nt.test(correct / n ~ primed, paired = T, data = d)\n\n\n\n    Paired t-test\n\ndata:  correct/n by primed\nt = 2.3045, df = 73, p-value = 0.02404\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.005741069 0.079201016\nsample estimates:\nmean of the differences \n             0.04247104 \n\nInstead of doing a probit regression, I’m going to do logistic regression. Therefore we define the prior on the log-odds scale. The log odds for the expected probability of .5 is 0. I prefer log-odds because–although complicated–they make sense, unlike standardized effect sizes. Note that the probit scale would also be fine as they are very similar.\nLet’s just get a quick intuition about effects in log-odds: The change in log odds from p = .5 to .55 is about 0.2.\n\n\ntibble(\n  rate = seq(0, 1, by = .01),\n  logit = arm::logit(rate)\n) %>%\n  ggplot(aes(rate, logit)) +\n  geom_line(size = 1) +\n  geom_segment(x = 0, xend = 0.55, y = .2, yend = .2, size = .4) +\n  geom_segment(x = 0, xend = 0.5, y = 0, yend = 0, size = .4) +\n  coord_cartesian(ylim = c(-2, 2), expand = 0)\n\n\n\n\nWe are cheating a little because we know these values, having simulated the data. However, log-odds are not straightforward (!), and this knowledge will allow us to specify better priors in this example. Let’s get the possible priors for this model by calling get_prior(). Notice that the model now includes id-varying “random” effects, and we model them from independent Gaussians by specifying || instead of | which would give a multivariate Gaussian on the varying effects.\n\n\nm2 <- bf(\n  correct | trials(n) ~ 0 + Intercept + prime +\n    (0 + Intercept + prime || id),\n  family = binomial(link = \"logit\")\n)\nget_prior(\n  m2,\n  data = d\n)\n\n\n                prior class      coef group resp dpar nlpar bound\n               (flat)     b                                      \n               (flat)     b Intercept                            \n               (flat)     b     prime                            \n student_t(3, 0, 2.5)    sd                                      \n student_t(3, 0, 2.5)    sd              id                      \n student_t(3, 0, 2.5)    sd Intercept    id                      \n student_t(3, 0, 2.5)    sd     prime    id                      \n       source\n      default\n (vectorized)\n (vectorized)\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n\nThe leftmost column gives the pre-specified defaults used by brms. Here are the priors we’ll specify. The most important pertains to prime, which is going to be the effect size in log-odds. Our prior for the log odds of the prime effect is going to be a Gaussian distribution centered on 0, with a standard deviation of .2, which is rather diffuse.\n\n\nPrior <- c(\n  set_prior(\"normal(0, 10)\", class = \"b\", coef = \"Intercept\"),\n  set_prior(\"cauchy(0, 10)\", class = \"sd\"),\n  set_prior(\"normal(0, .2)\", class = \"b\", coef = \"prime\")\n)\n\n\n\nThen we estimate the model using the specified priors.\n\n\nm2 <- brm(\n  m2,\n  prior = Prior,\n  sample_prior = TRUE,\n  iter = 1e4,\n  data = d,\n  cores = 4,\n  file = \"bayesfactormodel3\"\n)\n\n\n\nOK, so our results here will be different because we didn’t parameterize the prior on a standardized effect size because a) I don’t like standardized effect sizes, and b) I would have to play around with the Stan code, and this post is about brms. Anyway, here are the estimated parameters:\n\n\nsummary(m2)\n\n\n Family: binomial \n  Links: mu = logit \nFormula: correct | trials(n) ~ 0 + intercept + prime + (0 + intercept + prime || id) \n   Data: d (Number of observations: 148) \nSamples: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup samples = 20000\n\nGroup-Level Effects: \n~id (Number of levels: 74) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(intercept)     0.07      0.05     0.00     0.18 1.00     7067\nsd(prime)         0.12      0.08     0.01     0.30 1.00     5825\n              Tail_ESS\nsd(intercept)     9309\nsd(prime)         8073\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nintercept     0.01      0.05    -0.09     0.11 1.00    19281    15567\nprime         0.15      0.07     0.02     0.29 1.00    19914    15907\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nAnd our null-hypothesis density ratio:\n\n\nh2 <- hypothesis(m2, \"prime = 0\")\nh2\n\n\nHypothesis Tests for class b:\n   Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (prime) = 0     0.15      0.07     0.02     0.29       0.28      0.22    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nPriming effect of zero log-odds is 4 times less likely after seeing the data:\n\n\n1 / h2$hypothesis$Evid.Ratio\n\n\n[1] 3.522026\n\nThis is best illustrated by plotting the densities:\n\n\nplot(h2)\n\n\n\n\nConclusion\nRead the paper! Hopefully you’ll be able to use brms’ hypothesis() function to calculate bayes factors when needed.\n\nSupport this work\n\nSoftware used\nThe following software packages were used: R [Version 4.0.3; R Core Team (2020)] and the R-packages brms [Version 2.14.4; Bürkner (2017); Bürkner (2018)], dplyr [Version 1.0.4; Wickham et al. (2021)], forcats [Version 0.5.1; Wickham (2021a)], ggplot2 [Version 3.3.3; Wickham (2016)], kableExtra [Version 1.3.4; Zhu (2021)], knitr [Version 1.31; Xie (2015)], patchwork [Version 1.1.1; Pedersen (2020)], purrr [Version 0.3.4; Henry and Wickham (2020)], Rcpp [Version 1.0.6; Eddelbuettel and François (2011); Eddelbuettel and Balamuta (2018)], readr [Version 1.4.0; Wickham and Hester (2020)], scales [Version 1.1.1; Wickham and Seidel (2020)], stringr [Version 1.4.0; Wickham (2019)], tibble [Version 3.1.0; Müller and Wickham (2021)], tidyr [Version 1.1.3; Wickham (2021b)], and tidyverse [Version 1.3.0; Wickham et al. (2019)].\n\n\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\n———. 2018. “Advanced Bayesian Multilevel Modeling with the R Package brms.” The R Journal 10 (1): 395–411. https://doi.org/10.32614/RJ-2018-017.\n\n\nEddelbuettel, Dirk, and James Joseph Balamuta. 2018. “Extending extitR with extitC++: A Brief Introduction to extitRcpp.” The American Statistician 72 (1): 28–36. https://doi.org/10.1080/00031305.2017.1375990.\n\n\nEddelbuettel, Dirk, and Romain François. 2011. “Rcpp: Seamless R and C++ Integration.” Journal of Statistical Software 40 (8): 1–18. https://doi.org/10.18637/jss.v040.i08.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nMüller, Kirill, and Hadley Wickham. 2021. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nPedersen, Thomas Lin. 2020. Patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nStan Development Team. 2016. Stan: A c++ Library for Probability and Sampling, Version 2.15.0. http://mc-stan.org/.\n\n\nWagenmakers, Eric-Jan, Tom Lodewyckx, Himanshu Kuriyal, and Raoul Grasman. 2010. “Bayesian Hypothesis Testing for Psychologists: A Tutorial on the Savage–Dickey Method.” Cognitive Psychology 60 (3): 158–89. https://doi.org/10.1016/j.cogpsych.2009.12.001.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2021a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2021b. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, and Dana Seidel. 2020. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nXie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\nZhu, Hao. 2021. kableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra.\n\n\n\n\n",
    "preview": "posts/2017-03-21-bayes-factors-with-brms/2017-03-21-bayes-factors-with-brms_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-09T21:03:59+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2017-01-04-within-subject-scatter/",
    "title": "How to create within-subject scatter plots in R with ggplot2",
    "description": "Scatterplots can be a very effective form of visualization for data from within-subjects experiments. You'll often see within-subject data visualized as bar graphs (condition means, and maybe mean difference if you're lucky.) But alternatives exist, and today we'll take a look at within-subjects scatterplots. ratio method.",
    "author": [
      {
        "name": "Matti Vuorre",
        "url": "https://vuorre.netlify.com"
      }
    ],
    "date": "2017-01-04",
    "categories": [
      "statistics",
      "tutorial",
      "R",
      "visualization",
      "ggplot2"
    ],
    "contents": "\n\nContents\nSubject means\nWithin-subject scatterplots\nConclusion\n\nToday, we’ll take a look at creating a specific type of visualization for data from a within-subjects experiment (also known as repeated measures, but that can sometimes be a misleading label). You’ll often see within-subject data visualized as bar graphs (condition means, and maybe mean difference if you’re lucky.) But alternatives exist, and today we’ll take a look at within-subjects scatterplots.\nFor example, Ganis and Kievit (2015) asked 54 people to observe, on each trial, two 3-D shapes with various rotations and judge whether the two shapes were the same or not.\nThere were 4 angles (0, 50, 100, and 150 degree rotations), but for simplicity, today we’ll only look at items that were not rotated with respect to each other, and items rotated 50 degrees. The data are freely available (thanks!) in Excel format, and the below snippet loads the data and cleans into a useable format:\n\n\nif (!file.exists(\"data.zip\")) {\n  download.file(\"https://ndownloader.figshare.com/files/1878093\", \"data.zip\")\n}\nunzip(\"data.zip\")\nfiles <- list.files(\n  \"Behavioural_data/\",\n  pattern = \"sub[0-9]+.xlsx\", full.names = T\n)\ndat <- map(\n  files,\n  ~ read_xlsx(.x, range = \"A4:G100\", col_types = rep(\"text\", 7))\n) %>%\n  bind_rows(.id = \"id\")\ndat <- dat %>%\n  filter(angle %in% c(\"0\", \"50\")) %>%\n  transmute(\n    id = factor(id),\n    angle = factor(angle),\n    rt = as.numeric(Time),\n    accuracy = as.numeric(`correct/incorrect`)\n  )\n\n\n\n\n\nTable 1: Example data.\n\n\nid\n\n\nangle\n\n\nrt\n\n\naccuracy\n\n\n1\n\n\n0\n\n\n1355\n\n\n1\n\n\n1\n\n\n50\n\n\n1685\n\n\n1\n\n\n1\n\n\n50\n\n\n1237\n\n\n1\n\n\n1\n\n\n0\n\n\n1275\n\n\n1\n\n\n1\n\n\n50\n\n\n2238\n\n\n1\n\n\n1\n\n\n0\n\n\n1524\n\n\n1\n\n\nWe’ll focus on comparing the reaction times between the 0 degree and 50 degree rotation trials.\nSubject means\nWe’ll be graphing subjects’ means and standard errors, so we compute both first\n\n\ndat_sum <- group_by(dat, id, angle) %>%\n  summarize(\n    m = mean(rt, na.rm = T),\n    se = sd(rt, na.rm = TRUE) / sqrt(n())\n  )\n\n\n\nTable 2: Summary data\n\n\nid\n\n\nangle\n\n\nm\n\n\nse\n\n\n1\n\n\n0\n\n\n1512.12\n\n\n146.50\n\n\n1\n\n\n50\n\n\n2039.42\n\n\n133.74\n\n\n10\n\n\n0\n\n\n2784.39\n\n\n301.94\n\n\n10\n\n\n50\n\n\n3766.58\n\n\n337.51\n\n\n11\n\n\n0\n\n\n3546.30\n\n\n388.03\n\n\n11\n\n\n50\n\n\n4639.84\n\n\n281.78\n\n\n\n\ndat_sum %>%\n  ggplot(aes(x = angle, y = m)) +\n  stat_summary(\n    fun.data = mean_cl_normal, size = 1\n  ) +\n  geom_quasirandom(width = .1, shape = 1) +\n  scale_y_continuous(\"Mean RT\")\n\n\n\n\nThis figure shows quite clearly that the mean reaction time in the 50 degree angle condition was higher than in the 0 degree angle condition, and the spread across individuals in each condition. However, we often are specifically interested in the within-subject effect of condition, which would be difficult to visually display in this image. We could draw lines to connect each point, and the effect would then be visible as a “spaghetti plot,” but while useful, these plots may sometimes be a little overwhelming especially if there’s too many people (spaghetti is great but nobody likes too much of it!)\nWithin-subject scatterplots\nTo draw within-subjects scatterplots, we’ll need a slight reorganization of the data, such that it is in wide format with respect to the conditions.\n\n\ndat_sum_wide <- dat_sum %>% \n  pivot_wider(names_from = angle, values_from = c(m, se))\n\n\n\nTable 3: Summary data in wide format.\n\n\nid\n\n\nm_0\n\n\nm_50\n\n\nse_0\n\n\nse_50\n\n\n1\n\n\n1512.12\n\n\n2039.42\n\n\n146.50\n\n\n133.74\n\n\n10\n\n\n2784.39\n\n\n3766.58\n\n\n301.94\n\n\n337.51\n\n\n11\n\n\n3546.30\n\n\n4639.84\n\n\n388.03\n\n\n281.78\n\n\n12\n\n\n1251.04\n\n\n1767.54\n\n\n125.10\n\n\n211.44\n\n\n13\n\n\n1372.54\n\n\n2037.67\n\n\n86.25\n\n\n167.52\n\n\n14\n\n\n1231.92\n\n\n1666.25\n\n\n84.09\n\n\n126.10\n\n\nThen we can simply map the per-subject angle-means and standard errors to the X and Y axes. I think it’s important for these graphs to usually have a 1:1 aspect ratio, an identity line, and identical axes, which we add below.\n\n\nggplot(dat_sum_wide, aes(x = m_0, y = m_50)) +\n  # Equalize axes\n  scale_x_continuous(\"RT (0 degrees)\", limits = c(500, 5000)) +\n  scale_y_continuous(\"RT (50 degrees)\", limits = c(500, 5000)) +\n  # Identity line\n  geom_abline(size = .25) +\n  # 1:1 aspect ratio\n  theme(aspect.ratio = 1) +\n  # Points and errorbars\n  geom_point() +\n  geom_linerange(aes(ymin = m_50-se_50, ymax = m_50+se_50), size = .25) +\n  geom_linerange(aes(xmin = m_0-se_0, xmax = m_0+se_0), size = .25)\n\n\n\n\nThis plot shows each person (mean) as a point and their SEs as thin lines. The difference between conditions can be directly seen by how far from the diagonal line the points are. Were we to use CIs, we could also see subject-specific significant differences. Points above the diagonal indicate that the person’s (mean) RT was greater in the 50 degrees condition. All of the points lie below the identity line, indicating that the effect was as we predicted, and robust across individuals.\nThis is a very useful diagnostic plot that simultaneously shows the population- (or group-) level trend (are the points, on average, below or above the identity line?) and the expectation (mean) for every person (roughly, how far apart the points are from each other?). The points are naturally connected by their location, unlike in a bar graph where they would be connected by lines. Maybe you think it’s an informative graph; it’s certainly very easy to do in R with ggplot2. Also, I think it is visually very convincing, and doesn’t necessarily lead one to focus unjustly just on the group means: I am both convinced and informed by the graph.\nConclusion\nWithin-subject scatter plots are pretty common in some fields (psychophysics), but underutilized in many fields where they might have a positive impact on statistical inference. Why not try them out on your own data, especially when they’re this easy to do with R and ggplot2?\nRecall that for real applications, it’s better to transform or model reaction times with a skewed distribution. Here we used normal distributions just for convenience.\nFinally, this post was made possible by the Ganis and Kievit (2015) who generously have shared their data online.\n\nSupport this work\n\nSoftware used\nThe following software packages were used: R [Version 4.0.3; R Core Team (2020)] and the R-packages dplyr [Version 1.0.4; Wickham et al. (2021)], forcats [Version 0.5.1; Wickham (2021a)], ggbeeswarm [Version 0.6.0; Clarke and Sherrill-Mix (2017)], ggplot2 [Version 3.3.3; Wickham (2016)], ggstance [Version 0.3.5; Henry, Wickham, and Chang (2020)], kableExtra [Version 1.3.4; Zhu (2021)], knitr [Version 1.31; Xie (2015)], purrr [Version 0.3.4; Henry and Wickham (2020)], readr [Version 1.4.0; Wickham and Hester (2020)], readxl [Version 1.3.1; Wickham and Bryan (2019)], scales [Version 1.1.1; Wickham and Seidel (2020)], stringr [Version 1.4.0; Wickham (2019)], tibble [Version 3.1.0; Müller and Wickham (2021)], tidyr [Version 1.1.3; Wickham (2021b)], and tidyverse [Version 1.3.0; Wickham et al. (2019)].\n\n\n\n\nClarke, Erik, and Scott Sherrill-Mix. 2017. Ggbeeswarm: Categorical Scatter (Violin Point) Plots. https://CRAN.R-project.org/package=ggbeeswarm.\n\n\nGanis, Giorgio, and Rogier Kievit. 2015. “A New Set of Three-Dimensional Shapes for Investigating Mental Rotation Processes: Validation Data and Stimulus Set.” Journal of Open Psychology Data 3 (1). https://doi.org/10.5334/jopd.ai.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nHenry, Lionel, Hadley Wickham, and Winston Chang. 2020. Ggstance: Horizontal ’Ggplot2’ Components. https://CRAN.R-project.org/package=ggstance.\n\n\nMüller, Kirill, and Hadley Wickham. 2021. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2021a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2021b. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2019. Readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, and Dana Seidel. 2020. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nXie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\nZhu, Hao. 2021. kableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra.\n\n\n\n\n",
    "preview": "posts/2017-01-04-within-subject-scatter/2017-01-04-within-subject-scatter_files/figure-html5/scatter-1.png",
    "last_modified": "2021-03-09T21:34:31+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/",
    "title": "How to Compare Two Groups with Robust Bayesian Estimation in R",
    "description": "2017 will be the year when social scientists finally decided to diversify their applied statistics toolbox, and stop relying 100% on null hypothesis significance testing (NHST). A very appealing alternative to NHST is Bayesian statistics, which in itself contains many approaches to statistical inference. In this post, I provide an introductory and practical tutorial to Bayesian parameter estimation in the context of comparing two independent groups' data.",
    "author": [
      {
        "name": "Matti Vuorre",
        "url": "https://vuorre.netlify.com"
      }
    ],
    "date": "2017-01-02",
    "categories": [
      "statistics",
      "tutorial",
      "R",
      "brms"
    ],
    "contents": "\n\nContents\nIntroduction\nThe t in a t-test\nEqual variances t-test\nUnequal variances t-test\nDescribing the model(s) underlying the t-test(s)\n\nBayesian estimation of the t-test\nEqual variances model\nUnequal variances model\n\nRobust Bayesian Estimation\nConclusion\nSupport this work\nSoftware used\n\n\nIntroduction\nHappy New Year 2017 everybody! 2017 will be the year when social scientists finally decided to diversify their applied statistics toolbox, and stop relying 100% on null hypothesis significance testing (NHST). We now recognize that different scientific questions may require different statistical tools, and are ready to adopt new and innovative methods. A very appealing alternative to NHST is Bayesian statistics, which in itself contains many approaches to statistical inference. In this post, I provide an introductory and practical tutorial to Bayesian parameter estimation in the context of comparing two independent groups’ data.\nMore specifically, we’ll focus on the t-test. Everyone knows it, everyone uses it. Yet, there are (arguably) better methods for drawing inferences from two independent groups’ metric data (Kruschke 2013):\n\n“When data are interpreted in terms of meaningful parameters in a mathematical description, such as the difference of mean parameters in two groups, it is Bayesian analysis that provides complete information about the credible parameter values. Bayesian analysis is also more intuitive than traditional methods of null hypothesis significance testing (e.g., Dienes, 2011).” (Kruschke 2013)\n\nIn that article (“Bayesian estimation supersedes the t-test”) Kruschke (2013) provided clear and well-reasoned arguments favoring Bayesian parameter estimation over null hypothesis significance testing in the context of comparing two groups, a situation which is usually dealt with a t-test. It also introduced a “robust” model for comparing two groups, which modeled the data as t-distributed, instead of normal. The article provided R code for running the estimation procedures, which could be downloaded from the author’s website or as an R package.\nThe R code and programs work well for this specific application (estimating the robust model for one or two groups’ metric data). However, modifying the code to handle more complicated situations is not easy, and the underlying estimation algorithms don’t necessarily scale up to handle more complicated situations. Therefore, in this blog post I’ll introduce easy to use, free, open-source, state-of-the-art computer programs for Bayesian estimation, in the context of comparing two groups’ metric (continuous) data. The programs are available for the R programming language—so make sure you are familiar with R basics (e.g. here). I provide R code for t-tests and Bayesian estimation in R using the R package brms, which provides a concise front-end layer to Stan.\nThese programs supersede many older Bayesian inference programs because they are easy to use, fast, and are able to handle models with thousands of parameters. Learning to implement basic analyses such as t-tests, and Kruschke’s robust model, with these programs is very useful because you’ll then be able to do Bayesian statistics in practice, and will be prepared to understand and implement more complex models.\nUnderstanding the results of Bayesian estimation requires some knowledge of Bayesian statistics, of course, but since I cannot cover everything in this one post, I refer readers to excellent books on the topic: McElreath (2020), Kruschke (2014), Gelman et al. (2013).\nFirst, I’ll introduce the basic t-test in some detail, and then focus on understanding them as specific instantiations of linear models. If that sounds familiar, skip ahead to Bayesian Estimation of the t-test, where I introduce the brms package for estimating models using Bayesian methods. Following that, we’ll use “distributional regression” to obtain Bayesian estimates of the unequal variances t-test model. Finally, we’ll learn how to estimate the robust unequal variances model using brms.\nWe will use the following R packages:\n\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(broom)\nlibrary(brms)\nlibrary(tidyverse)\n\n\n\nThe t in a t-test\nWe’ll begin with t-tests, using example data from Kruschke’s paper (p. 577):\n\n“Consider data from two groups of people who take an IQ test. Group 1 (N1=47) consumes a “smart drug,” and Group 2 (N2=42) is a control group that consumes a placebo.\"\n\n\n\n\nThese data are visualized as histograms, below:\n\n\n\nFigure 1: Histograms of the two groups’ IQ scores.\n\n\n\nEqual variances t-test\nThese two groups’ IQ scores could be compared with a simple equal variances t-test (which you shouldn’t use; Lakens, 2015), also known as Student’s t-test.\n\n\nt.test(IQ ~ Group, data = d, var.equal = T)\n\n\n\n    Two Sample t-test\n\ndata:  IQ by Group\nt = -1.5587, df = 87, p-value = 0.1227\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.544155  0.428653\nsample estimates:\n  mean in group Control mean in group Treatment \n               100.3571                101.9149 \n\nWe interpret the t-test in terms of the observed t-value, and whether it exceeds the critical t-value. The critical t-value, in turn, is defined as the extreme \\(\\alpha / 2\\) percentiles of a t-distribution with the given degrees of freedom.\n\n\n\nFigure 2: t distribution with 87 degrees of freedom, and observed t-value. The dashed vertical lines indicate the extreme 2.5 percentiles. We would reject the null hypothesis of no difference if the observed t-value exceeded these percentiles.\n\n\n\nThe test results in an observed t-value of 1.56, which is not far enough in the tails of a t-distribution with 87 degrees of freedom to warrant rejecting the null hypothesis (given that we are using \\(\\alpha\\) = .05, which may or may not be an entirely brilliant idea).\nUnequal variances t-test\nNext, we’ll run the more appropriate, unequal variances t-test (also known as Welch’s t-test), which R gives by default:\n\n\nt.test(IQ ~ Group, data = d, var.equal = F)\n\n\n\n    Welch Two Sample t-test\n\ndata:  IQ by Group\nt = -1.6222, df = 63.039, p-value = 0.1098\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.4766863  0.3611848\nsample estimates:\n  mean in group Control mean in group Treatment \n               100.3571                101.9149 \n\nNote that while R gives Welch’s t-test by default, SPSS gives both. If you’re using SPSS, make sure to report the Welch’s test results, instead of the equal variances test. Here, the conclusion with respect to rejecting the null hypothesis of equal means is the same. However, notice that the results are numerically different, as they should, because these two t-tests refer to different models.\nIt is of course up to you, as a researcher, to decide whether you assume equal variances or not. But note that we almost always allow the means to be different (that’s the whole point of the test, really), while many treatments may just as well have an effect on the variances.\nThe first take-home message from today is that there are actually two t-tests, each associated with a different statistical model. And to make clear what the difference is, we must acquaint ourselves with the models.\nDescribing the model(s) underlying the t-test(s)\nWe don’t often think of t-tests (and ANOVAs) as models, but it turns out that they are just linear models disguised as “tests” (see here, here, and here). Recently, there has been a tremendous push for model/parameter estimation, instead of null hypothesis significance testing (Gigerenzer 2004; Cumming 2014; Kruschke 2014), so we will benefit from thinking about t-tests as linear models. Doing so will facilitate seamlessly expanding our models to handle more complicated situations.\nThe equal variances t-test models metric data with three parameters: Mean for group A, mean for group B, and one shared standard deviation (i.e. the assumption that the standard deviations are equal between the two groups.)\nWe call the metric outcome variable (IQ scores in our example) \\(y_{ik}\\), where \\(i\\) is a subscript indicating the \\(i^{th}\\) datum, and \\(k\\) indicates the \\(k^{th}\\) group. So \\(y_{19, 1}\\) would be the 19th datum, belonging to group 1. Then we specify that \\(y_{ik}\\) are normally distributed, \\(N(\\mu_{ik}, \\sigma)\\), where \\(\\mu_{ik}\\) indicates the mean of group \\(k\\), and \\(\\sigma\\) the common standard deviation.\n\\[y_{ik} \\sim N(\\mu_{ik}, \\sigma^2)\\]\nRead the formula as “Y is normally distributed with mean \\(\\mu_{ik}\\) (mu), and standard deviation \\(\\sigma\\) (sigma).” Note that the standard deviation \\(\\sigma\\) doesn’t have any subscripts: we assume it is the same for the groups.\nThe means for groups 0 and 1 are simply \\(\\mu_0\\) and \\(\\mu_1\\), respectively, and their difference (let’s call it \\(d\\)) is \\(d = \\mu_0 - \\mu_1\\). The 95% CI for \\(d\\) is given in the t-test output, and we can tell that it differs from the one given by Welch’s t-test.\nIt is unsurprising, then, that if we use a different model (the more appropriate unequal variances model), our inferences may be different. Welch’s t-test is the same as Student’s, except that now we assume (and subsequently estimate) a unique standard deviation \\(\\sigma_{ik}\\) for both groups.\n\\[y_{ik} \\sim N(\\mu_{ik}, \\sigma_{ik}^2)\\]\nThis model makes a lot of sense, because rarely are we in a situation to a priori decide that the variance of scores in Group A is equal to the variance of scores in Group B. If you use the equal variances t-test, you should be prepared to justify and defend this assumption. (Deciding between models—such as between these two t-tests—is one way in which our prior information enters and influences data analysis.)\nArmed with this knowledge, we can now see that “conducting a t-test” can be understood as estimating one of these two models. By estimating the model, we obtain t-values, degrees of freedom, and consequently, p-values.\nHowever, for the models described here, it can be easier to think of the t-test as a specific type of the general linear model. We can re-write the t-test in an equivalent way, but instead have a specific parameter for the difference in means by writing it as a linear model. The equal variance model can be written as\n\\[y_{ik} \\sim N(\\mu_{ik}, \\sigma^2)\\] \\[\\mu_{ik} = \\beta_0 + \\beta_1 Group_{ik}\\]\nHere, \\(\\sigma\\) is just as before, but we now model the mean with an intercept (control group’s mean, \\(\\beta_0\\)) and the effect of the treatment (\\(\\beta_1\\)). With this model, \\(\\beta_1\\) directly tells us the estimated difference in the two groups. And because it is a parameter in the model, it has an associated standard error, t-value, degrees of freedom, and a p-value. The model can be estimated in R with the following line of code:\n\n\nolsmod <- lm(IQ ~ Group, data = d)\n\n\n\nThe key input here is a model formula, which in R is specified as outcome ~ predictor (DV ~ IV). Using the lm() function, we estimated a linear model predicting IQ from an intercept (automatically included) and a Group parameter. I called this object olsmod for Ordinary Least Squares Model.\nR has it’s own model formula syntax, which is well worth learning. The formula in the previous model, IQ ~ Group means that we want to regress IQ on an intercept (which is implicitly included), and group (Group). Besides the formula, we only need to provide the data, which is contained in d.\nYou can verify that the results are identical to the equal variances t-test above.\n\n\nsummary(olsmod)\n\n\n\nCall:\nlm(formula = IQ ~ Group, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.9149  -0.9149   0.0851   1.0851  22.0851 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    100.3571     0.7263 138.184   <2e-16 ***\nGroupTreatment   1.5578     0.9994   1.559    0.123    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.707 on 87 degrees of freedom\nMultiple R-squared:  0.02717,   Adjusted R-squared:  0.01599 \nF-statistic:  2.43 on 1 and 87 DF,  p-value: 0.1227\n\nFocus on the GroupTreatment row in the estimated coefficients. Estimate is the point estimate (best guess) of the difference in means. t value is the observed t-value (identical to what t.test() reported), and the p-value (Pr(>|t|)) matches as well. The (Intercept) row refers to \\(\\beta_0\\), which is the control group’s mean.\nThis way of thinking about the model, where we have parameters for one group’s mean, and the effect of the other group, facilitates focusing on the important parameter, the difference, instead of individual means. However, you can of course compute the difference from the means, or the means from one mean and a difference.\nBayesian estimation of the t-test\nEqual variances model\nNext, I’ll illustrate how to estimate the equal variances t-test using Bayesian methods.\nEstimating this model with R, thanks to the Stan and brms teams, is as easy as the linear regression model we ran above. The most important function in the brms package is brm(), for Bayesian Regression Model(ing). The user needs only to input a model formula, just as above, and a data frame that contains the variables specified in the formula. brm() then translates the model into Stan language, and asks Stan to compile the model into C++ and draw samples from the posterior distribution. The result is an R object with the estimated results. We run the model and save the results to mod_eqvar for equal variances model:\n\n\nmod_eqvar <- brm(\n  IQ ~ Group,\n  data = d,\n  cores = 4,  # Use 4 cores for parallel processing\n  file = \"iqgroup\"  # Save results into a file\n)\n\n\n\nThe results can be viewed with summary():\n\n\nsummary(mod_eqvar)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: IQ ~ Group \n   Data: d (Number of observations: 89) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   100.34      0.72    98.92   101.78 1.00     4208     3116\nGroup         1.57      0.99    -0.38     3.56 1.00     3781     2917\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.71      0.36     4.07     5.47 1.00     3992     2799\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nNotice that the model contains three parameters, one of which is the shared standard deviation sigma. Compare the output of the Bayesian model to the one estimated with lm() (OLS):\n\n\nTable 1: Model results, left: OLS, right: brms.\n\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nbrms\n\n\nEstimate\n\n\nEst.Error\n\n\n(Intercept)\n\n\n100.36\n\n\n0.73\n\n\nIntercept\n\n\n100.34\n\n\n0.72\n\n\nGroupTreatment\n\n\n1.56\n\n\n1.00\n\n\nGroup\n\n\n1.57\n\n\n0.99\n\n\nThe point estimates (posterior means in the Bayesian model) and standard errors (SD of the respective posterior distribution) are pretty much identical.\nWe now know the models behind t-tests, and how to estimate the equal variances t-test using the t.test(), lm(), and brm() functions. We also know how to run Welch’s t-test using t.test(). However, estimating the general linear model version of the unequal variances t-test model is slightly more complicated, because it involves specifying predictors for \\(\\sigma\\), the standard deviation parameter.\nUnequal variances model\nWe only need a small adjustment to the equal variances model to specify the unequal variances model:\n\\[y_{ik} \\sim N(\\mu_{ik}, \\sigma_{ik})\\] \\[\\mu_{ik} = \\beta_0 + \\beta_1 Group_{ik}\\]\nNotice that we now have subscripts for \\(\\sigma\\), denoting that it varies between groups. In fact, we’ll write out a linear model for the standard deviation parameter.\n\\[\\sigma_{ik} = \\gamma_0 + \\gamma_1 Group_{ik}\\]\nThe model now includes, instead of a common \\(\\sigma\\), one parameter for Group 0’s standard deviation \\(\\gamma_0\\) (gamma), and one for the effect of Group 1 on the standard deviation \\(\\gamma_1\\), such that group 1’s standard deviation is \\(\\gamma_0 + \\gamma_1\\). Therefore, we have 4 free parameters, two means and two standard deviations. (The full specification would include prior distributions for all the parameters, but that topic is outside of the scope of this post.) brm() takes more complicated models by wrapping them inside bf() (short for brmsformula()), which is subsequently entered as the first argument to brm().\n\n\nuneq_var_frm <- bf(IQ ~ Group, sigma ~ Group)\n\n\n\nYou can see that the formula regresses IQ on Group, such that we’ll have an intercept (implicitly included), and an effect of Group 1. We also model the standard deviation sigma on Group.\n\n\nmod_uneqvar <- brm(\n  uneq_var_frm,\n  data = d,\n  cores = 4,\n  file = \"iqgroup-uv\"\n)\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: IQ ~ Group \n         sigma ~ Group\n   Data: d (Number of observations: 89) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         100.35      0.38    99.60   101.05 1.00     5225     2457\nsigma_Intercept     0.94      0.12     0.73     1.17 1.00     3689     2586\nGroup               1.55      0.99    -0.34     3.54 1.00     2343     2608\nsigma_Group         0.87      0.16     0.55     1.18 1.00     3699     2299\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe model’s output contains our 4 parameters. Intercept is the mean for group 0, Group 1 is the “effect of group 1.” The sigma_Intercept is the standard deviation of Group 0, sigma_Group is the effect of group 1 on the standard deviation (the SD of Group 1 is sigma_Intercept + sigma_Group). The sigmas are implicitly modeled through a log-link (because they must be positive). To convert them back to the scale of the data, they need to be exponentiated. After taking the exponents of the sigmas, the results look like this:\n\n\nTable 2: Posterior summary after transformation\n\n\nParameter\n\n\nEstimate\n\n\nEst.Error\n\n\nQ2.5\n\n\nQ97.5\n\n\nIntercept\n\n\n100.35\n\n\n0.38\n\n\n99.60\n\n\n101.05\n\n\nsigma_Intercept\n\n\n2.57\n\n\n0.30\n\n\n2.07\n\n\n3.23\n\n\nGroup\n\n\n1.55\n\n\n0.99\n\n\n-0.34\n\n\n3.54\n\n\nsigma_Group\n\n\n2.41\n\n\n0.38\n\n\n1.74\n\n\n3.24\n\n\nKeep in mind that the parameters refer to Group 0’s mean (Intercept) and SD (sigma), and the difference between groups in those values (Group) and (sigma_Group). We now have fully Bayesian estimates of the 4 parameters of the unequal variances t-test model. Finally, let’s move on to the “Robust Bayesian Estimation” model.\nRobust Bayesian Estimation\nKruschke’s robust model is a comparison of two groups, using five parameters: One mean for each group, one standard deviation for each group, just as in the unequal variances model above. The fifth parameter is a “normality” parameter, \\(\\nu\\) (nu), which means that we are now using a t-distribution to model the data. Using a t-distribution to model the data, instead of a Gaussian, means that the model is less sensitive to extreme values. Here’s what the model looks like:\n\\[y_{ik} \\sim T(\\nu, \\mu_{ik}, \\sigma_{ik})\\]\nRead the above formula as “Y are random draws from a t-distribution with ‘normality’ parameter \\(\\nu\\), mean \\(\\mu_{ik}\\), and standard deviation \\(\\sigma_{ik}\\).” We have linear models for the means and standard deviations, as above.\nThis model, as you can see, is almost identical to the unequal variances t-test, but instead uses a t distribution (we assume data are t-distributed), and includes the normality parameter. Using brm() we can still use the unequal variances model, but have to specify the t-distribution. We do this by specifying the family argument to be student (as in Student’s t)\n\n\nmod_robust <- brm(\n  bf(IQ ~ Group, sigma ~ Group),\n  family = student,\n  data = d,\n  cores = 4,\n  file = \"iqgroup-robust\"\n)\n\n\n\n\n Family: student \n  Links: mu = identity; sigma = log; nu = identity \nFormula: IQ ~ Group \n         sigma ~ Group\n   Data: d (Number of observations: 89) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         100.52      0.20   100.14   100.91 1.00     5007     2964\nsigma_Intercept    -0.00      0.19    -0.39     0.37 1.00     3837     3059\nGroup               1.02      0.43     0.16     1.85 1.00     2380     2415\nsigma_Group         0.68      0.25     0.20     1.16 1.00     3549     2746\n\nFamily Specific Parameters: \n   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nnu     1.84      0.46     1.17     2.95 1.00     2799     1942\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nYou can compare the results to those in Kruschke’s paper (2013, p.578) to verify that they are nearly identical. There are small discrepancies because of limited number of posterior samples, and because the paper reported posterior modes whereas we focused on means.\nFinally, here is how to estimate the model using the original code (Kruschke & Meredith, 2015):\n\n\nlibrary(BEST)\nBEST <- BESTmcmc(group_0, group_1)\n\n\n\nI didn’t actually run that code because after numerous attempts, I was unable to install the rjags package that BEST depends on.\nConclusion\nWell, that ended up much longer than what I intended. The aim was both to illustrate the ease of Bayesian modeling in R using brms, and highlight the fact that we can easily move from simple t-tests to more complex (and possibly better) models.\nIf you’ve followed through, you should be able to conduct Student’s (equal variances) and Welch’s (unequal variances) t-tests in R, and to think about those tests as instantiations of general linear models. Further, you should be able to estimate these models using Bayesian methods.\nYou should now also be familiar with Kruschke’s robust model for comparing two groups’ metric data, and be able to implement it a few lines of R code. This model found credible differences between two groups, although the frequentist t-tests and models reported p-values well above .05. That should be motivation enough to try robust (Bayesian) models on your own data.\n\nSupport this work\n\nSoftware used\nThe following software packages were used: R [Version 4.0.3; R Core Team (2020)] and the R-packages brms [Version 2.14.4; Bürkner (2017); Bürkner (2018)], broom [Version 0.7.5.9000; Robinson, Hayes, and Couch (2021)], coda [Version 0.19.4; Plummer et al. (2006)], dplyr [Version 1.0.4; Wickham et al. (2021)], forcats [Version 0.5.1; Wickham (2021a)], ggplot2 [Version 3.3.3; Wickham (2016)], kableExtra [Version 1.3.4; Zhu (2021)], knitr [Version 1.31; Xie (2015)], purrr [Version 0.3.4; Henry and Wickham (2020)], Rcpp [Version 1.0.6; Eddelbuettel and François (2011); Eddelbuettel and Balamuta (2018)], readr [Version 1.4.0; Wickham and Hester (2020)], scales [Version 1.1.1; Wickham and Seidel (2020)], stringr [Version 1.4.0; Wickham (2019)], tibble [Version 3.1.0; Müller and Wickham (2021)], tidybayes [Version 2.3.1; Kay (2020)], tidyr [Version 1.1.3; Wickham (2021b)], and tidyverse [Version 1.3.0; Wickham et al. (2019)].\n\n\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\n———. 2018. “Advanced Bayesian Multilevel Modeling with the R Package brms.” The R Journal 10 (1): 395–411. https://doi.org/10.32614/RJ-2018-017.\n\n\nCumming, Geoff. 2014. “The New Statistics Why and How.” Psychological Science 25 (1): 7–29. https://doi.org/10.1177/0956797613504966.\n\n\nEddelbuettel, Dirk, and James Joseph Balamuta. 2018. “Extending extitR with extitC++: A Brief Introduction to extitRcpp.” The American Statistician 72 (1): 28–36. https://doi.org/10.1080/00031305.2017.1375990.\n\n\nEddelbuettel, Dirk, and Romain François. 2011. “Rcpp: Seamless R and C++ Integration.” Journal of Statistical Software 40 (8): 1–18. https://doi.org/10.18637/jss.v040.i08.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third Edition. Boca Raton: Chapman and Hall/CRC.\n\n\nGigerenzer, Gerd. 2004. “Mindless Statistics.” The Journal of Socio-Economics, Statistical Significance, 33 (5): 587–606. https://doi.org/10.1016/j.socec.2004.09.033.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nKay, Matthew. 2020. tidybayes: Tidy Data and Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151.\n\n\nKruschke, John K. 2013. “Bayesian Estimation Supersedes the t Test.” Journal of Experimental Psychology: General 142 (2): 573–603. https://doi.org/10.1037/a0029146.\n\n\n———. 2014. Doing Bayesian Data Analysis: A Tutorial Introduction with R. 2nd Edition. Burlington, MA: Academic Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. CRC Texts in Statistical Science. Boca Raton: Taylor and Francis, CRC Press.\n\n\nMüller, Kirill, and Hadley Wickham. 2021. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nPlummer, Martyn, Nicky Best, Kate Cowles, and Karen Vines. 2006. “CODA: Convergence Diagnosis and Output Analysis for MCMC.” R News 6 (1): 7–11. https://journal.r-project.org/archive/.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2021. Broom: Convert Statistical Objects into Tidy Tibbles.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2021a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2021b. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, and Dana Seidel. 2020. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nXie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\nZhu, Hao. 2021. kableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra.\n\n\n\n\n",
    "preview": "posts/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms_files/figure-html5/dataplot1-1.png",
    "last_modified": "2021-03-09T21:10:55+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 614
  },
  {
    "path": "posts/2016-12-06-order-ggplot-panel-plots/",
    "title": "How to arrange ggplot2 panel plots",
    "description": "Arrange your visual display of information to maximize your figures' impact.",
    "author": [
      {
        "name": "Matti Vuorre",
        "url": "https://vuorre.netlify.com"
      }
    ],
    "date": "2016-12-06",
    "categories": [
      "data science",
      "R",
      "visualization"
    ],
    "contents": "\n\nContents\nOrder panels on mean value\nOrder panels on other parameters\nSupport this work\nSoftware used\n\n\nPanel plots are a common name for figures showing every person’s (or whatever your sampling unit is) data in their own panel. This plot is sometimes also known as “small multiples,” although that more commonly refers to plots that illustrate interactions. Here, I’ll illustrate how to add information to a panel plot by arranging the panels according to some meaningful value.\nHere’s an example of a panel plot, using the sleepstudy data set from the lme4 package.\n\n\nlibrary(knitr)\nlibrary(scales)\nlibrary(tidyverse)\n\n\n\n\n\ndata(sleepstudy, package = \"lme4\")\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:9) +\n  facet_wrap(\"Subject\", labeller = label_both)\n\n\n\n\nOn the x-axis is days of sleep deprivation, and y-axis is an aggregate measure of reaction time across a number of cognitive tasks. Reaction time increases as a function of sleep deprivation. But the order of the panels is entirely uninformative, they are simply arranged in increasing order of subject ID number, from top left to bottom right. Subject ID numbers are rarely informative, and we would therefore like to order the panels according to some other fact about the individual participants.\nOrder panels on mean value\nLet’s start by ordering the panels on the participants’ mean reaction time, with the fastest participant in the upper-left panel.\nStep 1 is to add the required information to the data frame used in plotting. For a simple mean, we can actually use a shortcut in step 2, so this isn’t required.\nStep 2: Convert the variable used to separate the panels into a factor, and order it based on the mean reaction time.\nThe key here is to use the reorder() function. You’ll first enter the variable that contains the groupings (i.e. the subject ID numbers), and then values that will be used to order the grouping variables. Finally, here you can use a shortcut to base the ordering on a function of the values, such as the mean, by entering it as the third argument.\n\n\nsleepstudy <- mutate(\n  sleepstudy,\n  Subject = reorder(Subject, Reaction, mean)\n)\n\n\n\nNow if we use Subject to create the subplots, they will be ordered on the mean reaction time. I’ll make the illustration clear by also drawing the person-means with small arrows.\n\n\n\nOrder panels on other parameters\nIt might also be useful to order the panels based on a value from a model, such as the slope of a linear regression. This is especially useful in making the heterogeneity in the sample easier to see. For this, you’ll need to fit a model, grab the subject-specific slopes, order the paneling factor, and plot. I’ll illustrate with a multilevel regression using lme4.\n\n\n# Step 1: Add values to order on into the data frame\nlibrary(lme4)\nmod <- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n# Create a data frame with subject IDs and coefficients\ncoefs <- coef(mod)$Subject %>%\n  rownames_to_column(\"Subject\")\nnames(coefs) <- c(\"Subject\", \"Intercept\", \"Slope\")\n# Join to main data frame by Subject ID\nsleepstudy <- left_join(sleepstudy, coefs, by = \"Subject\")\n\n# Step 2: Reorder the grouping factor\nsleepstudy <- mutate(\n  sleepstudy,\n  Subject = reorder(Subject, Slope)\n)\n\n\n\nThen, I’ll plot the data also showing the fitted lines from the multilevel model:\n\n\n\nHopefully you’ll find this helpful.\n\nSupport this work\n\nSoftware used\nThe following software packages were used: R [Version 4.0.3; R Core Team (2020)] and the R-packages brms [Version 2.14.4; Bürkner (2017); Bürkner (2018)], dplyr [Version 1.0.4; Wickham et al. (2021)], forcats [Version 0.5.1; Wickham (2021a)], ggplot2 [Version 3.3.3; Wickham (2016)], kableExtra [Version 1.3.4; Zhu (2021)], knitr [Version 1.31; Xie (2015)], lme4 [Version 1.1.26; Bates et al. (2015)], Matrix [Version 1.3.2; Bates and Maechler (2021)], patchwork [Version 1.1.1; Pedersen (2020)], purrr [Version 0.3.4; Henry and Wickham (2020)], Rcpp [Version 1.0.6; Eddelbuettel and François (2011); Eddelbuettel and Balamuta (2018)], readr [Version 1.4.0; Wickham and Hester (2020)], scales [Version 1.1.1; Wickham and Seidel (2020)], stringr [Version 1.4.0; Wickham (2019)], tibble [Version 3.1.0; Müller and Wickham (2021)], tidyr [Version 1.1.3; Wickham (2021b)], and tidyverse [Version 1.3.0; Wickham et al. (2019)].\n\n\n\n\nBates, Douglas, and Martin Maechler. 2021. Matrix: Sparse and Dense Matrix Classes and Methods. https://CRAN.R-project.org/package=Matrix.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\n———. 2018. “Advanced Bayesian Multilevel Modeling with the R Package brms.” The R Journal 10 (1): 395–411. https://doi.org/10.32614/RJ-2018-017.\n\n\nEddelbuettel, Dirk, and James Joseph Balamuta. 2018. “Extending extitR with extitC++: A Brief Introduction to extitRcpp.” The American Statistician 72 (1): 28–36. https://doi.org/10.1080/00031305.2017.1375990.\n\n\nEddelbuettel, Dirk, and Romain François. 2011. “Rcpp: Seamless R and C++ Integration.” Journal of Statistical Software 40 (8): 1–18. https://doi.org/10.18637/jss.v040.i08.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nMüller, Kirill, and Hadley Wickham. 2021. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nPedersen, Thomas Lin. 2020. Patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2021a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2021b. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, and Dana Seidel. 2020. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nXie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\nZhu, Hao. 2021. kableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra.\n\n\n\n\n",
    "preview": "posts/2016-12-06-order-ggplot-panel-plots/2016-12-06-order-ggplot-panel-plots_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-03-09T21:05:45+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2016-09-29-bayesian-meta-analysis/",
    "title": "Bayesian Meta-Analysis with R, Stan & brms",
    "description": "Meta-analysis is a special case of Bayesian multilevel modeling",
    "author": [
      {
        "name": "Matti Vuorre",
        "url": "https://vuorre.netlify.com"
      }
    ],
    "date": "2016-09-29",
    "categories": [
      "statistics",
      "R",
      "brms",
      "tutorial"
    ],
    "contents": "\n\nContents\nIntroduction\nThe data\nThe model\nEstimation with metafor\n\nThe switch to Bayes\nEstimation with brms\n\nComparing results\nForest plot\n\nDiscussion\nBayesian multilevel modeling\nSupport this work\nSoftware used\n\n\nIntroduction\nRecently, there’s been a lot of talk about meta-analysis, and here I would just like to quickly show that Bayesian multilevel modeling nicely takes care of your meta-analysis needs, and that it is easy to do in R with the rstan and brms packages. As you’ll see, meta-analysis is a special case of Bayesian multilevel modeling when you are unable or unwilling to put a prior distribution on the meta-analytic effect size estimate.\nThe idea for this post came from Wolfgang Viechtbauer’s website, where he compared results for meta-analytic models fitted with his great (frequentist) package metafor and the swiss army knife of multilevel modeling, lme4. It turns out that even though you can fit meta-analytic models with lme4, the results are slightly different from traditional meta-analytic models, because the experiment-wise variances are treated slightly differently.\nHere are the packages we’ll use:\n\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(metafor)\nlibrary(scales)\nlibrary(lme4)\nlibrary(brms)\nlibrary(tidyverse)\n\n\n\nThe data\nHere I’ll only focus on a simple random effects meta-analysis of effect sizes, and will use the same example data as in the aforementioned website. The data are included in the metafor package, and describe the relationship between conscientiousness and medication adherence. The effect sizes are r to z transformed correlations.\n\n\n\n\n\nTable 1: Example data (dat.molloy2014 in metafor package).\n\n\nstudy\n\n\nyear\n\n\nni\n\n\nri\n\n\nyi\n\n\nvi\n\n\nsei\n\n\nAxelsson et al. (2009)\n\n\n2009\n\n\n109\n\n\n0.19\n\n\n0.19\n\n\n0.01\n\n\n0.10\n\n\nAxelsson et al. (2011)\n\n\n2011\n\n\n749\n\n\n0.16\n\n\n0.16\n\n\n0.00\n\n\n0.04\n\n\nBruce et al. (2010)\n\n\n2010\n\n\n55\n\n\n0.34\n\n\n0.35\n\n\n0.02\n\n\n0.14\n\n\nChristensen et al. (1995)\n\n\n1995\n\n\n72\n\n\n0.27\n\n\n0.28\n\n\n0.01\n\n\n0.12\n\n\nChristensen et al. (1999)\n\n\n1999\n\n\n107\n\n\n0.32\n\n\n0.33\n\n\n0.01\n\n\n0.10\n\n\nCohen et al. (2004)\n\n\n2004\n\n\n65\n\n\n0.00\n\n\n0.00\n\n\n0.02\n\n\n0.13\n\n\nDobbels et al. (2005)\n\n\n2005\n\n\n174\n\n\n0.17\n\n\n0.18\n\n\n0.01\n\n\n0.08\n\n\nEdiger et al. (2007)\n\n\n2007\n\n\n326\n\n\n0.05\n\n\n0.05\n\n\n0.00\n\n\n0.06\n\n\nInsel et al. (2006)\n\n\n2006\n\n\n58\n\n\n0.26\n\n\n0.27\n\n\n0.02\n\n\n0.13\n\n\nJerant et al. (2011)\n\n\n2011\n\n\n771\n\n\n0.01\n\n\n0.01\n\n\n0.00\n\n\n0.04\n\n\nMoran et al. (1997)\n\n\n1997\n\n\n56\n\n\n-0.09\n\n\n-0.09\n\n\n0.02\n\n\n0.14\n\n\nO’Cleirigh et al. (2007)\n\n\n2007\n\n\n91\n\n\n0.37\n\n\n0.39\n\n\n0.01\n\n\n0.11\n\n\nPenedo et al. (2003)\n\n\n2003\n\n\n116\n\n\n0.00\n\n\n0.00\n\n\n0.01\n\n\n0.09\n\n\nQuine et al. (2012)\n\n\n2012\n\n\n537\n\n\n0.15\n\n\n0.15\n\n\n0.00\n\n\n0.04\n\n\nStilley et al. (2004)\n\n\n2004\n\n\n158\n\n\n0.24\n\n\n0.24\n\n\n0.01\n\n\n0.08\n\n\nWiebe & Christensen (1997)\n\n\n1997\n\n\n65\n\n\n0.04\n\n\n0.04\n\n\n0.02\n\n\n0.13\n\n\nThe model\nWe are going to fit a random-effects meta-analysis model to these observed effect sizes and their standard errors. Here’s what this model looks like, loosely following notation from the R package Metafor’s manual (p.6):\n\\[y_i \\sim N(\\theta_i, \\sigma_i^2)\\]\nwhere each recorded effect size, \\(y_i\\) is a draw from a normal distribution which is centered on that study’s “true” effect size \\(\\theta_i\\) and has standard deviation equal to the study’s observed standard error \\(\\sigma_i\\).\nOur next set of assumptions is that the studies’ true effect sizes approximate some underlying effect size in the (hypothetical) population of all studies. We call this underlying population effect size \\(\\mu\\), and its standard deviation \\(\\tau\\), such that the true effect sizes are thus distributed:\n\\[\\theta_i \\sim N(\\mu, \\tau^2)\\]\nWe now have two interesting parameters: \\(\\mu\\) tells us, all else being equal, what I may expect the “true” effect to be, in the population of similar studies. \\(\\tau\\) tells us how much individual studies of this effect vary.\nI think it is most straightforward to write this model as yet another mixed-effects model (metafor manual p.6):\n\\[y_i \\sim N(\\mu + \\theta_i, \\sigma^2_i)\\]\nwhere \\(\\theta_i \\sim N(0, \\tau^2)\\), studies’ true effects are normally distributed with between-study heterogeneity \\(\\tau^2\\). The reason this is a little confusing (to me at least), is that we know the \\(\\sigma_i\\)s (this being the fact that separates meta-analysis from other more common regression modeling).\nEstimation with metafor\nSuper easy!\n\n\nlibrary(metafor)\nma_out <- rma(data = dat, yi = yi, sei = sei, slab = dat$study)\nsummary(ma_out)\n\n\n\nRandom-Effects Model (k = 16; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc \n  8.6096  -17.2191  -13.2191  -11.8030  -12.2191   \n\ntau^2 (estimated amount of total heterogeneity): 0.0081 (SE = 0.0055)\ntau (square root of estimated tau^2 value):      0.0901\nI^2 (total heterogeneity / total variability):   61.73%\nH^2 (total variability / sampling variability):  2.61\n\nTest for Heterogeneity:\nQ(df = 15) = 38.1595, p-val = 0.0009\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub \n  0.1499  0.0316  4.7501  <.0001  0.0881  0.2118  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe switch to Bayes\nSo far so good, we’re strictly in the realm of standard meta-analysis. But I would like to propose that instead of using custom meta-analysis software, we simply consider the above model as just another regression model, and fit it like we would any other (multilevel) regression model. That is, using Stan, usually through the brms interface. Going Bayesian allows us to assign prior distributions on the population-level parameters \\(\\mu\\) and \\(\\tau\\), and we would usually want to use some very mildly regularizing priors. Here we proceed with brms’ default priors (which I print below with the output)\nEstimation with brms\nHere’s how to fit this model with brms:\n\n\nbrm_out <- brm(\n  yi | se(sei) ~ 1 + (1 | study), \n  data = dat, \n  cores = 4,\n  file = \"metaanalysismodel\"\n)\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: yi | se(sei) ~ 1 + (1 | study) \n   Data: dat (Number of observations: 16) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPriors: \nIntercept ~ student_t(3, 0.2, 2.5)\nsd ~ student_t(3, 0, 2.5)\n\nGroup-Level Effects: \n~study (Number of levels: 16) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.10      0.04     0.04     0.18 1.00     1182     1722\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.15      0.04     0.08     0.22 1.00     2175     2094\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00 1.00     4000     4000\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nThese results are the same as the ones obtained with metafor. Note the Student’s t prior distributions, which are diffuse enough not to exert influence on the posterior distribution.\nComparing results\nWe can now compare the results of these two estimation methods. Of course, the Bayesian method has a tremendous advantage, because it results in a full distribution of plausible values.\n\n\n\nFigure 1: Histogram of samples from the posterior distribution of the average effect size (top left) and the variability (top right). Bottom left displays the multivariate posterior distribution of the average (x-axis) and the standard deviation (y-axis), light colors indicating increased plausibility of values. For each plot, the dashed lines display the maximum likelihood point estimate, and 95% confidence limits (only the point estimate is displayed for the multivariate figure.)\n\n\n\nWe can see from the numeric output, and especially the figures, that these modes of inference yield the same numerical results. Keep in mind though, that the Bayesian estimates actually allow you to discuss probabilities, and generally the things that we’d like to discuss when talking about results.\nFor example, what is the probability that the average effect size is greater than 0.2? About eight percent:\n\n\nhypothesis(brm_out, \"Intercept > 0.2\")\n\n\nHypothesis Tests for class b:\n             Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (Intercept)-(0.2) > 0    -0.05      0.04     -0.1     0.01       0.08      0.08     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nForest plot\nThe forest plot displays the entire posterior distribution of each \\(\\theta_i\\). The meta-analytic effect size \\(\\mu\\) is also displayed in the bottom row. I’ll show a considerable amount of code here so that you can create your own forest plots from brms output:\n\n\nlibrary(tidybayes)\nlibrary(ggdist)\n# Study-specific effects are deviations + average\nout_r <- spread_draws(brm_out, r_study[study,term], b_Intercept) %>% \n  mutate(b_Intercept = r_study + b_Intercept) \n# Average effect\nout_f <- spread_draws(brm_out, b_Intercept) %>% \n  mutate(study = \"Average\")\n# Combine average and study-specific effects' data frames\nout_all <- bind_rows(out_r, out_f) %>% \n  ungroup() %>%\n  # Ensure that Average effect is on the bottom of the forest plot\n  mutate(study = fct_relevel(study, \"Average\")) %>% \n  # tidybayes garbles names so fix here\n  mutate(study = str_replace_all(study, \"\\\\.\", \" \"))\n# Data frame of summary numbers\nout_all_sum <- group_by(out_all, study) %>% \n  mean_qi(b_Intercept)\n# Draw plot\nout_all %>%   \n  ggplot(aes(b_Intercept, study)) +\n  # Zero!\n  geom_vline(xintercept = 0, size = .25, lty = 2) +\n  stat_halfeye(.width = c(.8, .95), fill = \"dodgerblue\") +\n  # Add text labels\n  geom_text(\n    data = mutate_if(out_all_sum, is.numeric, round, 2),\n    aes(label = str_glue(\"{b_Intercept} [{.lower}, {.upper}]\"), x = 0.75),\n    hjust = \"inward\"\n  ) +\n  # Observed as empty points\n  geom_point(\n    data = dat %>% mutate(study = str_replace_all(study, \"\\\\.\", \" \")), \n    aes(x=yi), position = position_nudge(y = -.2), shape = 1 \n  )\n\n\n\n\nFigure 2: Forest plot of the example model’s results. Filled points and intervals are posterior means and 80/95% Credible Intervals. Empty points are observed effect sizes.\n\n\n\nFocus on Moran et al. (1997)’s observed effect size (the empty circle): This is an anomalous result compared to all other studies. One might describe it as incredible, and that is indeed what the bayesian estimation procedure has done, and the resulting posterior distribution is no longer equivalent to the observed effect size. Instead, it is shrunken toward the average effect size. Now look at the table above, this study only had 56 participants, so we should be more skeptical of this study’s observed ES, and perhaps we should then adjust our beliefs about this study in the context of other studies. Therefore, our best guess about this study’s effect size, given all the other studies is no longer the observed mean, but something closer to the average across the studies.\nIf this shrinkage business seems radical, consider Quine et al. (2012). This study had a much greater sample size (537), and therefore a smaller SE. It was also generally more in line with the average effect size estimate. Therefore, the observed mean ES and the mean of the posterior distribution are pretty much identical. This is also a fairly desirable feature.\nDiscussion\nThe way these different methods are presented (regression, meta-analysis, ANOVA, …), it is quite easy for a beginner, like me, to lose sight of the forest for the trees. I also feel that this is a general experience for students of applied statistics: Every experiment, situation, and question results in a different statistical method (or worse: “Which test should I use?”), and the student doesn’t see how the methods relate to each other. So I think focusing on the (regression) model is key, but often overlooked in favor of this sort of decision tree model of choosing statistical methods (McElreath 2020).\nAccordingly, I think we’ve ended up in a situation where meta-analysis, for example, is seen as somehow separate from all the other modeling we do, such as repeated measures t-tests. In fact I think applied statistics in Psychology may too often appear as an unconnected bunch of tricks and models, leading to confusion and inefficient implementation of appropriate methods.\nBayesian multilevel modeling\nAs I’ve been learning more about statistics, I’ve often noticed that some technique, applied in a specific set of situations, turns out to be a special case of a more general modeling approach. I’ll call this approach here Bayesian multilevel modeling (McElreath 2020). If you are forced to choose one statistical method to learn, it should be Bayesian multilevel modeling, because it allows you to do and understand most things, and allows you to see how similar all these methods are, under the hood.\nSupport this work\n\nSoftware used\nThe following software packages were used in this blog post: R [Version 4.0.3; R Core Team (2020)] and the R-packages brms [Version 2.14.4; Bürkner (2017); Bürkner (2018)], dplyr [Version 1.0.4; Wickham et al. (2021)], forcats [Version 0.5.1; Wickham (2021a)], ggdist [Version 2.4.0; Kay (2021)], ggplot2 [Version 3.3.3; Wickham (2016)], kableExtra [Version 1.3.4; Zhu (2021)], knitr [Version 1.31; Xie (2015)], lme4 [Version 1.1.26; Bates et al. (2015)], Matrix [Version 1.3.2; Bates and Maechler (2021)], metafor [Version 2.4.0; Viechtbauer (2010)], purrr [Version 0.3.4; Henry and Wickham (2020)], Rcpp [Version 1.0.6; Eddelbuettel and François (2011); Eddelbuettel and Balamuta (2018)], readr [Version 1.4.0; Wickham and Hester (2020)], scales [Version 1.1.1; Wickham and Seidel (2020)], stringr [Version 1.4.0; Wickham (2019)], tibble [Version 3.1.0; Müller and Wickham (2021)], tidybayes [Version 2.3.1; Kay (2020)], tidyr [Version 1.1.3; Wickham (2021b)], and tidyverse [Version 1.3.0; Wickham et al. (2019)].\n\n\n\nBates, Douglas, and Martin Maechler. 2021. Matrix: Sparse and Dense Matrix Classes and Methods. https://CRAN.R-project.org/package=Matrix.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\n———. 2018. “Advanced Bayesian Multilevel Modeling with the R Package brms.” The R Journal 10 (1): 395–411. https://doi.org/10.32614/RJ-2018-017.\n\n\nEddelbuettel, Dirk, and James Joseph Balamuta. 2018. “Extending extitR with extitC++: A Brief Introduction to extitRcpp.” The American Statistician 72 (1): 28–36. https://doi.org/10.1080/00031305.2017.1375990.\n\n\nEddelbuettel, Dirk, and Romain François. 2011. “Rcpp: Seamless R and C++ Integration.” Journal of Statistical Software 40 (8): 1–18. https://doi.org/10.18637/jss.v040.i08.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nKay, Matthew. 2020. tidybayes: Tidy Data and Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151.\n\n\n———. 2021. ggdist: Visualizations of Distributions and Uncertainty. https://doi.org/10.5281/zenodo.3879620.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. CRC Texts in Statistical Science. Boca Raton: Taylor and Francis, CRC Press.\n\n\nMüller, Kirill, and Hadley Wickham. 2021. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting Meta-Analyses in R with the metafor Package.” Journal of Statistical Software 36 (3): 1–48. https://www.jstatsoft.org/v36/i03/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2021a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2021b. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, and Dana Seidel. 2020. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nXie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\nZhu, Hao. 2021. kableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra.\n\n\n\n\n",
    "preview": "posts/2016-09-29-bayesian-meta-analysis/2016-09-29-bayesian-meta-analysis_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-09T22:33:24+00:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 960
  }
]
